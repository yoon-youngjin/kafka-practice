# 카프카 컨슈머 애플리케이션 개발

## 카프카 컨슈머 소개

<img width="527" alt="image" src="https://github.com/yoon-youngjin/kafka-practice/assets/83503188/46f5c324-3363-4655-b08b-da0542c11b8d">

프로듀서가 전송한 데이터는 카프카 브로커에 적재된다. 컨슈머는 적재된 데이터를 사용하기 위해 브로커로부터 데이터를 가져와서 필요한 처리를 한다.
예를 들어, 마케팅 문자를 고객에게 보내는 기능이 있다면 컨슈머는 토픽으로부터 고객 데이터를 가져와서 문자 발송 처리를 하게 된다.

### 컨슈머 내부 구조 

<img width="582" alt="image" src="https://github.com/yoon-youngjin/kafka-practice/assets/83503188/edc59489-1955-42ee-b984-e4221940064d">

카프카 클러스터(리더 파티션이 포함된 브로카)에서 데이터를 보내면 Fetcher가 우선적으로 데이터를 받는다. 
이후에 poll()를 통해서 ConsumerRecords 를 받는다. 

위 그림에서 보다싶이 poll()을 호출하기 전에 이미 데이터를 가져온다. 그러한 이유로 poll()이 조금 늦게 호출되더라도(레코드 처리속도가 늦더라도) 이미 데이터는 가져온 상태이기 때문에 처리속도를 유지할 수 있다.

- Fetcher : 리더 파티션으로부터 레코드들을 미리 가져와서 대기
- poll() : Fetcher에 있는 레코드들을 리턴하는 메서드
- ConsumerRecords : 처리하고자하는 레코드들의 모음. 오프셋이 포함되어 있음 
  - 프로듀서에서 보낸 레코드는 브로커에 저장될 때 오프셋이 포함되어 있다.
  - 컨슈머에서 해당 레코드 처리가 완료되었다면 **커밋**을 통해서 특정 오프셋까지 데이터가 처리되었음을 판단할 수 있다.

## 컨슈머 그룹

<img width="710" alt="image" src="https://github.com/yoon-youngjin/kafka-practice/assets/83503188/08e02b3f-bb4b-4d8a-9dc0-eb20bb70728e">

컨슈머 그룹으로 운영하는 방법은 컨슈머를 각 컨슈머 그룹으로부터 격리된 환경에서 안전하게 운영할 수 있도록 도와주는 카프카의 독특한 방식이다.
컨슈머 그룹으로 묶인 컨슈머들은 토픽의 1개 이상 파티션들에 할당되어 데이터를 가져갈 수 있다. 컨슈머 그룹으로 묶인 컨슈머가 토픽을 구독해서 데이터를 가져갈 때, 1개의 파티션은 최대 1개의 컨슈머에 할당 가능하다.
그리고 1개 컨슈머는 여러 개의 파티션에 할당될 수 있다. 이러한 특징으로 컨슈머 그룹의 컨슈머 개수는 가져가고자 하는 토픽의 파티션 개수보다 같거나 작아야 한다.

컨슈머 그룹에 포함된 컨슈머들은 일반적으로 동일한 로직을 가진 컨슈머(컨슈머 애플리케이션, 쓰레드)들이다. 
즉, 특정 파티션의 레코드들을 온전히 특정 컨슈머 그룹이 처리하고 싶을 때 (목적에 따라서 나눔) 사용하는 것이 컨슈머 그룹이다.

컨슈머 그룹의 컨슈머가 subscribe()를 통해서 토픽을 구독하면 토픽의 전체 파티션에서 데이터를 가져가게 된다.
위와 같이 토픽을 구독해서 데이터를 처리할 때는 파티션 개수만큼 컨슈머 애플리케이션의 수를 늘리면 최대 성능을 활용할 수 있다.

### 컨슈머 그룹의 컨슈머가 파티션 개수보다 많을 경우

<img width="326" alt="image" src="https://github.com/yoon-youngjin/kafka-practice/assets/83503188/a5a81f98-7fac-4db5-b5dc-9a59244963b2">

만약 4개의 컨슈머로 이루어진 컨슈머 그룹으로 3개의 파티션을 가진 토픽에서 데이터를 가져가기 위해 할당하면 1개의 컨슈머는 파티션을 할당받지 못하고 유휴(idle) 상태로 남게 된다.
파티션을 할당받지 못한 컨슈머는 스레드만 차지하고 실질적인 데이터 처리를 하지 못하므로 애플리케이션 실행에 있어 불필요한 스레드로 남게 된다.

### 컨슈머 그룹을 활용하는 이유

<img width="366" alt="image" src="https://github.com/yoon-youngjin/kafka-practice/assets/83503188/f38c4bf7-b9ee-4c79-9a87-bd4361d24c9f">

운영 서버의 주요 리소스인 CPU, 메모리 정보를 수집하는 데이터 파이프라인을 구축한다고 가정해보자.
실시간 리소스를 시간순으로 확인하기 위해서 데이터를 엘라스틱서치에 저장하고 이와 동시에 대용량 적재를 위해 하둡을 적재할 것이다.
만약 카프카를 활용한 파이프라인이 아니라면 서버에서 실행되는 리소스 수집 및 전송 에이전트는 수집한 리소스를 엘라스틱 서치와 하둡에 적재하기 위해 동기적으로 적재를 요청할 것이다.
이렇게 동기로 실행되는 에이전트는 엘라스틱서치 또는 하둡 둘 중 하나에 장애가 발생한다면 더는 적재가 불가능할 수 있다.

<img width="449" alt="image" src="https://github.com/yoon-youngjin/kafka-practice/assets/83503188/860e4aa4-c8c4-4862-aec9-33bb7ad1063c">

이러한 커플링을 끊기 위해서 위와 같이 각기 다른 저장소에 저장하는 컨슈머를 다른 컨슈머 그룹으로 묶음으로써 각 저장소의 장애에 격리되어 운영할 수 있다.
따라서 엘라스틱서치의 장애로 인해 더는 적재가 되지 못하더라도 하둡으로 데이터를 적재하는 데에는 문제가 없다. 
엘라스틱서치의 장애가 해소되면 엘라스틱서치로 적재하는 컨슈머의 컨슈머 그룹은 마지막으로 적재 완료한 데이터 이후부터 다시 적재를 수행하여 최종적으로 모두 정상화될 것이다.

파티션 개수만큼 컨슈머 개수를 늘려서 데이터 처리량을 늘릴 수도 있고, 필요에 따라서 굳이 처리량을 늘릴 필요가 없다면 컨슈머 개수를 줄일 수도 있다.

## 리밸런싱

- 카프카에 독특한 failover 방식

<img width="471" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/40d7ccf7-348e-42ee-b826-44e4e261ceff">

컨슈머 그룹으로 이루어진 컨슈머들 중 일부 컨슈머에 장애가 발생하면, 장애가 발생한 컨슈머에 할당된 파티션은 장애가 발생하지 않은 컨슈머에 소유권이 넘어간다.
이러한 과정을 **리밸런싱**이라고 부른다. 리밸런싱은 크게 두 가지 상황에서 일어나는데, 첫 번째는 컨슈머가 추가되는 상황이고 두 번째는 컨슈머가 제외되는 상황이다. 
이슈가 발생한 컨슈머를 컨슈머 그룹에서 제외하여 모든 파티션이 지속적으로 데이터를 처리할 수 있도록 가용성을 높여준다. 리밸런싱은 컨슈머가 데이터를 처리하는 도중에 언제든지 발생할 수 있으므로 데이터 처리 중 발생한 리밸런싱에 대응하는 코드를 작성해야 한다.

파티션 개수가 적을수록 리밸런싱 시간이 적게 소요되는데, 파티션 개수가 많은 경우에는 리밸런싱 시간이 크게 소요된다.
따라서 리밸런싱 상황은 장애와 비슷한 상황인데, 이를 위한 로직을 짜야한다.

## 커밋

<img width="491" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/1b5d8581-7324-4a1a-801a-ab28784b94a5">

> 브로커의 파티션에서 컨슈머가 poll()을 통해서 데이터를 가져가고 commit을 통해서 n번째 데이터를 처리했음을 알려주는것

컨슈머는 카프카 브로커로부터 데이터를 어디까지 가져갔는지 커밋을 통해 기록한다. 특정 토픽의 파티션을 어떤 컨슈머 그룹이 몇 번째 가져갔는지 카프카 브로커 내부에서 사용되는 내부 토픽(__consumer_offsets)에 기록된다.
컨슈머 동작 이슈가 발생하여 __consumer_offsets 토픽에 어느 레코드까지 읽어갔는지 오프셋 커밋이 기록되지 못했다면 데이터 처리의 중복이 발생할 수 있다. 
그러므로 데이터 처리의 중복이 발생하지 않게 하기 위해서는 컨슈머 애플리케이션이 오프셋 커밋을 정상적으러 처리했는지 검증해야만 한다.

## 어싸이너(Assignor)

컨슈머와 파티션 할당 정책은 컨슈머의 Assigner에 의해 결정된다. 카프카에서는 RangeAssignor, RoundRobinAssignor, StickyAssignor를 제공한다.

> 카프카 2.5.0은 RangeAssignor가 기본값

- RangeAssignor : 각 토픽에서 파티션을 숫자로 정렬, 컨슈머를 사전 순서로 정렬하여 할당
- RoundRobinAssignor : 모든 파티션을 컨슈머에서 번갈아가며 할당
- StickyAssignor : 최대한 파티션을 균등하게 배분하면서 할당

대부분의 환경에서는 토픽의 파티션이 0, 1, 2가 존재한다면 컨슈머 그룹을 운영한다면 컨슈머를 1대1 매칭되도록 구성한다.
즉, 어떤 어싸이너를 사용하던 상관없을 경우가 많다. (어차피 1대1 맵핑)

## 컨슈머 주요 옵션 소개

### 컨슈머 필수 옵션 

- bootstrap.servers : 프로듀서가 데이터를 전송할 대상 카프카 클러스터에 속한 브로커의 호스트 이름:포트를 1개 이상 작성한다. 2개 이상 브로커 정보를 입력하여 일부 브로커에 이슈가 발생하더라도 접속하는 데에 이슈가 없도록 설정 가능하다.
- key.deserializer : 레코드의 메시지 키를 역직렬화하는 클래스를 지정한다.
- value.deserializer : 레코드의 메시지 값을 역직렬화하는 클래스를 지정한다.

### 컨슈머 선택 옵션

- group.id : 컨슈머 그룹 아이디를 지정한다. subscribe() 메서드로 토픽을 구독하여 사용할 때는 해당 옵션이 필수이다. 기본값은 null
- auto.offset.reset : 컨슈머 그룹이 특정 파티션을 읽을 때 저장된 컨슈머 오프셋이 없는 경우(한번도 커밋을 X) 어느 오프셋부터 읽을지 선택하는 옵션이다. 이미 컨슈머 오프셋이 있다면 이 옵션값은 무시된다. 기본값은 latest이다.
  - 가장 최신의 데이터를 읽을지 가장 오래된 데이터를 읽을지를 정한다.
  - latest : 가장 최근에 넣은(숫자가 가장 높은) 오프셋부터 읽기 시작한다.
  - earliest : 가장 오래전에 넣은(숫자가 가장 낮은) 오프셋부터 읽기 시작한다.
  - none : 컨슈머 그룹이 커밋한 기록이 있는지 찾아본다. 만약 커밋 기록이 없으면 오류를 반환하고, 커밋 기록이 있다면 기존 커밋 기록 이후 오프셋부터 읽기 시작한다.
  - 특정 토픽에 대해서 컨슈머가 이미 커밋을 했다면 해당 옵션은 무시된다. 즉, 최초에 만든 컨슈머를 운영하는 시점에 팔요한 옵션이다.
- enable.auto.commit : 자동 커밋으로 할지 수동 커밋으로 할지 선태갛낟. 기본값은 true 이다.
  - 만약 메시지를 처리할 때마다 오프셋을 커밋하고 싶다면 해당 값을 false로 한다.
- auto.commit.interval.ms : 자동 커밋일 경우 오프셋 커밋 간격을 지정한다. 기본값은 5000(5초)이다.
  - 카프카 컨슈머는 처리한 메시지의 오프셋을 주기적으로 커밋하여, 해당 메시지가 성공적으로 처리되었음을 카프카 브로커에 알리는데, 이를 통해 컨슈머가 다시 시작될 때 마지막으로 커밋된 오프셋부터 메시지를 읽기 시작한다.
  - 해당 값을 낮추면 컨슈머는 더 자주 오프셋을 커밋하게 된다. 반면 이 값을 높게 설정하면 오프셋 커밋이 덜 자주 발생하므로, 네트워크 트래픽이 감소하지만 잠재적으로 더 많은 메시지를 재처리해야 할 수도 있다. 
- max.poll.records : poll() 메서드를 통해 반환되는 레코드 개수를 지정한다. 기본값은 500이다.
- session.timeout.ms : 컨슈머가 브로커와 연결이 끊기는 최대 시간이다. 기본값은 10000(10초)이다.
  - 해당 값을 너무 길게하면 비정상동작을 판단하는데 너무 긴 시간을 소요하게되고, 너무 짧게하면 문제가 없는데도 불필요한 리밸런싱이 발생할 수 있다.
- hearbeat.interval.ms : 하트비트를 전송하는 시간 간격이다. 기본값은 3000(3초)이다.
- max.poll.interval.ms : poll() 메서드를 호출하는 간격의 최대 시간. 기본값은 300000(5분)이다.
  - 컨슈머가 poll() 메서드를 호출하고나서 메서드를 처리하고 있는데 설정값을 넘어가서도 다음 poll()이 호출되지 않으면 문제가 있다고 판단하여 리밸런싱을 실행한다. 
- isolation.level : 트랜잭션 프로듀서가 레코드를 트랜잭션 단위로 보낼 경우 사용한다.

## 동기 오프셋 커밋 컨슈머

poll() 메서드가 호출된 이후에 commitSync() 메서드를 호출하여 오프셋 커밋을 명시적으로 수행할 수 있다.
commitSync()는 poll() 메서드로 받은 가장 마지막 레코드의 오프셋을 기준으로 커밋한다.
동기 오프셋 커밋을 사용할 경우에는 poll() 메서드로 받은 모든 레코드의 처리가 끝난 이후 commitSync() 메서드를 호출해야 한다.

```kotlin
while (true) {
  val records = consumer.poll(Duration.ofSeconds(1))
  for (record in records) {
    logger.info("record:{}", record)
  }
  cosumer.commitSync()
}
```
- 만약 poll()을 통해서 99번까지의 오프셋을 가진 레코드를 받았다면 99번까지 모두 완료된 이후에 커밋을 진행한다.

### 동기 오프셋 커밋(레코드 단위) 컨슈머

```kotlin
while (true) {
  val records = consumer.poll(Duration.ofSeconds(1))
  val currentOffset: Map<TopicPartition, OffsetAndMetadata>  = HashMap<>()
  
  for (record in records) {
      logger.info("record:{}", record)
      currentOffset.put(
          TopicPartition(record.topic(), record.partition()),
          OffsetAndMetadata(record.offset() + 1, null)
      )
      cosumer.commitSync() 
  }
}
```
- 위와 같이하면 레코드마다 커밋을 실행할 수 있는데, 해당 부분은 레코드마다 네트워크 통신이 발생하므로 컨슈머 성능 및 브로커에 영향을 미칠수 있다.

### 비동기 오프셋 커밋 컨슈머

동기 오프셋 커밋을 사용할 경우 브로커로부터 커밋 응답을 기다리는 동안 데이터 처리가 일시적으로 중단 되기 때문에 더 많은 데이터를 처리하기 위해서 비동기 오프셋 커밋을 사용할 수 있다. 
비동기 오프셋 커밋은 commitAsync() 메서드를 호출하여 사용할 수 있다.

```kotlin
while (true) {
  val records = consumer.poll(Duration.ofSeconds(1))
  for (record in records) {
    logger.info("record:{}", record)
  }
  cosumer.commitASync()
}
```

### 비동기 오프셋 커밋 콜백

```kotlin
while (true) {
  val records = consumer.poll(Duration.ofSeconds(1))
  for (record in records) {
    logger.info("record:{}", record)
  }
  cosumer.commitASync(object : OffsetCommitCallback() {
      fun onComplete(offsets : Map<TopicPartion, OffsetAndMetadata>, e : Exception) {
          ...
      }
  })
}
```

## 리밸런스 리스너를 가진 컨슈머

리밸런스 발생을 감지하기 위해 카프카 라이브러리는 ConsumerRebalanceListener 인터페이스를 지원한다. ConsumerRebalanceListener 인터페이스로 구현된 클래스는 onPartitionAssigned() 메서드와 onPartitionRevoked() 메서드로 이루어져 있다.

- onPartitionAssigned() : 리밸런스가 끝난 뒤에 파티션이 할당 완료돠면 호출되는 메서드
- onPartitionRevoked() : 리밸런스가 시작되기 직전에 호출되는 메서드
  - 마지막으로 처리한 레코드를 기준으로 커밋을 하기 위해서는 리밸런스가 시작하기 직전에 커밋을 하면 되므로 onPartitionRevoked()에 커밋을 구현하여 처리할 수 있다.

```kotlin
class RebalanceListener : ConsumerRebalanceListener {
    override fun onPartitionsRevoked(partitions: MutableCollection<TopicPartition>?) {
        logger.warn("Partitions are assigned : " + partitions.toString())
    }

    override fun onPartitionsAssigned(partitions: MutableCollection<TopicPartition>?) {
        logger.warn("Partitions are revoked : " + partitions.toString())
    }
}

consumer.subscribe(listOf(TOPIC_NAME), RebalanceListener())
```

## 파티션 할당 컨슈머

기본적으로 subscribe() 상황에서 토픽을 구독하는 컨슈머 그룹으로 운영할 수도 있지만 직접 토픽에 대해서 파티션을 각각 할당해서 해당 파티션만 데이터를 처리하도록 구성할수도 있다.

```kotlin
consumer.assign(Collections.singleton(TopicPartition(TOPIC_NAME, TOPIC_NUMBER)))
```

## 컨슈머 애플리케이션의 안전한 종료

컨슈머 애플리케이션은 안전하게 종료되어야 한다. 정상적으로 종료되지 않은 컨슈머는 세션 타임아웃이 발생할때까지 컨슈머 그룹에 남게 된다.
즉, 해당 컨슈머가 정상적으로 종료되었음을 명시적으로 브로커에 알리지 않으면 리밸런싱 과정이 늦어지게된다. 

컨슈머를 안전하게 종료하기 위해 KafkaConsumer 클래스는 wakeup() 메서드를 지원한다. wakeup() 메서드를 실행하여 KafkaConsumer 인스턴스를 안전하게 종료할 수 있다.
wakeup() 메서드가 실행된 이후 poll() 메서드가 호출되면 WakeupException 예외가 발생한다. WakeupException 예외를 받은 뒤에는 데이터 처리를 위해 사용한 자원들을 해제하면 된다.

```kotlin
try {
  while (true) {
    val records = consumer.poll(Duration.ofSeconds(1))
    for (record in records) {
      logger.info("record:{}", record)
    }
    cosumer.commitSync()
  }
} catch (e : WakeupException) {
    logger.warn("Wakeup consumer")
} finally {
    consumer.close()
}
```

- poll() 메서드를 실행할 때 마다 wakeup을 확인한다.

## 멀티스레드 컨슈머

컨슈머는 기본적으로 1thread로 동작한다.
컨슈머를 여러 개 운영하는 방법은 스레드 1개를 가진 여러 개의 프로세스를 띄우거나 혹은 프로세스 하나에 여러 개의 스레드를 할당하는 방법이 있다.

<img width="462" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/d66c1af8-6859-48b7-b074-3e00d5281afb">

카프카 처리량을 늘리기 위해 파티션과 컨슈머 개수를 늘려서 운영할 수 있다. 파티션을 여러 개로 운영하는 경우 데이터를 병렬처리하기 위해서 파티션 개수와 컨슈머 개수를 동일하게 맞추는 것이 가장 좋은 방법이다.
토픽의 파티션은 1개 이상으로 이루어져 있으며 1개 파티션은 1개 컨슈머가 할당되어 데이터를 처리할 수 있다. 파티션 개수가 n개 라면 동일 파티션 그룹으로 묶인 컨슈머 스레드를 최대 n개 ㄱ우녕항 수 있다.
그러므로 n개 스레드를 가진 1개의 프로세스를 운영하거나 1개의 스레드를 가진 프로세스를 n개 운영하는 방법도 있다.

## 컨슈머 랙

<img width="422" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/f48ebe33-2ca6-4d88-8baa-674f3e3dbab6">

컨슈머 랙(LAG)은 파티션의 최신 오프셋(LOG-END-OFFSET)과 컨슈머 오프셋(CURRENT-OFFSET) 간의 차이(위 그림에서는 2)다. 프로듀서는 계속해서 새로운 데이터를 파티션에 저장하고 컨슈머는 자신이 처리할 수 있는 만큼 데이터를 가져간다.
컨슈머 랙은 컨슈머가 정상 동작하는지 여부를 확인할 수 있기 떄문에 컨슈머 애플리케이션을 운영한다면 필수적으로 모니터링해야 하는 지표이다.

<img width="448" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/2937ca04-34b3-42e3-a41e-fcbf3e93b101">

컨슈머 랙은 컨슈머 그룹과 토픽, 파티션별로 생성된다. 1개의 토픽에 3개의 파티션이 있고 1개의 컨슈머 그룹이 토픽을 구독하여 데이터를 가져가면 컨슈머 랙은 총 3개가 된다.
만약 동일한 토픽을 구독하는 컨슈머 그룹이 2개라면 총 컨슈머 랙은 6개가 된다.

### 프롣듀서와 컨슈머의 데이터 처리량 

<img width="727" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/3e26b414-050d-4d11-86ee-a930c663c4ea">

프로듀서가 보내는 데이터양이 컨슈머의 데이터 처리량보다 크다면 컨슈머 랙은 늘어난다.
반대로 프로듀서가 보내는 데이터양이 컨슈머의 데이터 처리량보다 적으면 컨슈머 랙은 줄어들고 최솟값은 0으로 지연이 없음을 뜻한다.
따라서 일반적으로 컨슈머의 처리 속도가 프로듀서의 발행 속도보다 빠른것이 이상적인 상황이다.

### 컨슈머 랙 모니터링

컨슈머 랙을 모니터링하는 것은 카프카를 통한 데이터 파이프라인을 운영하는데에 핵심적인 역할을 한다.
컨슈머 랙을 모니터링함으로써 컨슈머의 장애를 확인할 수 있고 파티션 개수를 정하는 데에 참고할 수 있기 때문이다.

예를 들어 프로듀서가 발행하는 데이터가 늘어남으로써 컨슈머 랙이 늘어나면 지연을 줄이기 위해 파티션 개수와 컨슈머 개수를 늘려서 병렬처리량을 늘리는 방법을 사용해볼 수 있다.

**파티션 이슈**

프로듀서의 데이터양이 일정함에도 불구하고 컨슈머의 장애로 인해 컨슈머 랙이 증가할 수도 있다. 컨슈머는 파티션 개수만큼 늘려서 병령처리하며 파티션마다 컨슈머가 할당되어 데이터를 처리한다.
예를 들어, 2개의 파티션으로 구성된 토픽에 2개의 컨슈머가 각각 할당되어 데이터를 처리한다고 가정해보자. 프로듀서가 보내는 데이터양은 동일한데 파티션 1번의 컨슈머 랙이 늘어나는 상황이 발생한다면 1번 파티션에 할당된 컨슈머에 이슈가 발생했음을 유추할 수 있다.

## 컨슈머 랙을 모니터링하는 방법

### 1. 카프카 명령어 사용

kafka-consumer-groups.sh 명령어를 사용하면 컨슈머 랙을 포함한 특정 컨슈머 그룹의 상태를 확인할 수 있다.
컨슈머 랙을 확인하기 위한 가장 기초적인 방법으로 다음과 같이 명령어를 사용하면 된다.

```text
./bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group my-group --describe
```

카프카 명령어를 통해 컨슈머 랙을 확인하는 방법은 일회성에 그치고 지표를 지속적으로 기록하고 모니터링하기에는 부족하다.
따라서 명령어를 통해 카프카 랙을 확인하는 것은 테스트용 카프카에서 주로 사용한다.

### 2. metrics() 메서드 사용

컨슈머 애플리케이션에서 KafkaConsumer 인스턴스의 metrics() 메서드를 활용하면 컨슈머 랙 지표를 확인할 수 있다. 
컨슈머 인스턴스가 제공하는 컨슈머 랙 관련 모니터링 지표는 3가지로 records-lag-max, records-lag, records-lag-avg이다.

**metrics() 사용 이슈**

- 컨슈머가 정상 동작할 경우에만 확인할 수 있다. 만약 컨슈머 애플리케이션이 비정상적으로 종료되면 더는 컨슈머 랙을 모니터링할 수 없다.
- 모든 컨슈머 애플리케이션에 컨슈머 랙 모니터링 코드를 중복해서 작성해야 한다. 컨슈머 애플리케이션을 여러 종류로 운영할 경우 각기 다른 컨슈머 애플리케이션에 metrics() 메서드를 호출하여 컨슈머 랙을 수집하는 로직을 중복해서 넣어야 한다. 
  - 특정 컨슈머 그룹에 해당하는 애플리케이션이 수집하는 컨슈머 랙은 자기 자신 컨슈머 그룹에 대한 컨슈머 랙만 한정되기 때문
- 컨슈머 랙을 모니터링하는 코드를 추가할 수 없는 카프카 서드 파티 애플리케이션(logstash, fluentD, telegraf, ...)의 컨슈머 랙 모니터링이 불가능하다.

### 3. 외부 모니터링 툴 사용

컨슈머 래을 모니터링하는 가장 최선의 방법은 외부 모니터링 툴을 사용하는 것이다. 데이터 독, 컨플루언트 컨트롤 센터와 같은 카프카 클러스터 종합 모니터링 툴을 사용하면 카프카 운영에 필요한 다양한 지표를 모니터링할 수 있다.
모니터링 지표에는 컨슈머 랙도 포함되어 있기 때문에 클러스터 모니터링과 컨슈머 랙을 함께 모니터링하기에 적합하다.
컨슈머 랙 모니터링만을 위한 툴로 오픈소스로 공개되어 있는 버로우(Burrow)가 있다.

## 카프카 버로우

링크드인에서 개발하여 오픈소스로 공개한 컨슈머 랙 체크 툴로써 REST API를 통해 컨슈머 그룹 별로 컨슈머 랙을 확인할 수 있다.
외부 모니터링 툴을 사용하면 카프카 클러스토에 연결된 모든 컨슈머, 토픽들의 랙 정보를 한번에 모니터링할 수 있다는 장점이 있다. 또한, 모니터링 툴들은 클러스터와 연동되어 컨슈머의 데이터 처리완느 별개로 지표를 수집하기 때문에 데이터를 활용하는 프로듀서나 컨슈머 동작에 영향을 미치지 않는다는 장점도 있다.

버로우는 다수의 카프카 클러스터를 동시에 연결하여 컨슈머 랙을 확인한다.
기업 환경에서는 카프카 클러스터를 2개 이상 구축하고 운영하는 경우가 많기 때문에 한 번의 설정으로 다수의 카프카 클러스터 컨슈머 랙을 확인할 수 있다는 장점이 있다.

### REST API

컨슈머 그룹별 컨슈머 랙을 조회할 수 있다.

<img width="722" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/594b9626-94ed-4531-8c97-9ab780ec861c">

### 컨슈머 랙 이슈 판별

<img width="390" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/a5940e3d-1c60-4f9c-8be5-09872721eb0d">

버로우 기능 중 가장 돋보이는 것은 컨슈머와 파티션의 상태를 단순히 컨슈머 랙의 임계치(threshhold)로 나타내지 않았다는 점이다.
특정 파티션의 컨슈머 랙이 특정 시점에 100만이 넘었다고 컨슈머 또는 파티션에 이슈가 있다고 단정 지을 수는 없다. 왜냐하면 프로듀서가 데이터를 많이 보내면 일시적으로 임계치가 넘어가는 현상이 발생할 수도 있기 때문이다.
컨슈머 애플리케이션을 운영할 때 컨슈머 랙이 임계치에 도달할 때마다 알람을 받는 것은 무의미한 일이다.

지연이 잠깐 발생하고 다시 줄어드는 경우 (일시적인 경우)에는 대응하지 않을 수 있다. 

**컨슈머 랙 평가(Evaluation)**

버로우에서 위와 같은 판단이 가능한 이유는 임계치가 아닌 슬라이딩 윈도우 계산을 통해 문제가 생긴 파티션과 컨슈머의 상태를 표현한다. 
이렇게 버로우에서 컨슈머 랙의 상태를 표현하는 것을 컨슈머 랙 평가라고 부른다. 

컨슈머 랙과 파티션의 오프셋을 시간순서대로 슬라이딩 윈도우로 계산하면 상태가 정해진다. 결과적으로 파티션의 상태를 OK, STALLED, STOPPED로 표현하고 컨슈머의 상태를 OK, WARNING, ERROR로 표현한다. 

**정상적인 경우**

<img width="431" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/38d5367e-55bf-432e-bdc3-123d6859caff">

일반적으로 컨슈머를 운영하면서 자주 볼 수 있는 그래프이다. 프로듀서가 지속적으로 데이터를 추가하기 때문에 최신 오프셋은 계속해서 증가한다.
컨슈머는 데이터 처리를 하면서 때때로 컨슈머 랙이 증가하지만 다시 컨슈머 랙이 0으로 줄어드는 추이를 볼 수 있다. 이런 경우 컨슈머가 정상적으로 동작한다고 볼 수 있으며 버로우는 파티션 OK, 컨슈머도 OK 상태를 나타낸다.

**컨슈머 처리량 이슈**

<img width="419" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/6d2d3276-f8ec-463d-85b0-3fbecaca0ecc">

프로듀서가 추가하는 최신 오프셋에 비해 컨슈머 오프셋이 따라가지 못하는 추이를 볼 수 있다. 최신 오프셋과 컨슈머 오프셋의 거리가 계속벌어지면서 컨슈머 랙은 지속적으로 증가한다.
이러한 그래프가 나오는 이유는 컨슈머의 데이터 처리량이 프로듀서가 보내는 데이터양에 비해 적기 때문이다. 이 경우에 버로우는 파티션 OK, 컨슈머는 WARNING 상태로 나타낸다.

이러한 경우 파티션과 컨슈머 개수를 늘림으로써 처리량을 늘리는 방법을 취해볼 수 있다.

**컨슈머 이슈**

<img width="421" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/88f8b328-f1a6-408a-8e16-0813c7968209">

프로듀서에 의해 최신 오프셋 지속적으로 증가하고 있지만 컨슈머 오프셋이 멈춘 것은 확인할 수 있다. 이로 인해 컨슈머 랙이 급격하게 증가하는 모습이다.
컨슈머가 어떠한 이유로 데이터를 더는 가져가지 않는 것(커밋 X)이다. 이 경우에 버로우는 파티션 STALLED, 컨슈머는 ERROR 상태로 나타낸다. 컨슈머 상태가 ERROR인 경우에는 컨슈머가 확실히 비정상 동작하고 있으므로 이메일, SMS, 슬랙 등의 알람을 받고 즉각 조치해야한다.

### 컨슈머 랙 모니터링 아키텍처

<img width="321" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/b1c0cf48-9452-4a36-9452-dbd64a1aa0bb">

버로우를 통해 컨슈머 랙을 모니터링할 때는 이미 지나간 컨슈머 랙을 개별적으로 모니터링하기 위해서 별개의 저장소와 대시보드를 사용하는 것이 효과적이다. 컨슈머 랙 모니터링을 위해 시용할 수 있는 저장소와 대시보드는 다양하다.
- 위 그림은 텔레그래프를 통해 ES로 데이터를 수집하고 그라파나로 모니터링하고 있다.

---

## Consumer Group과 Consumer

모든 컨슈머들은 단 하나의 컨슈머 그룹에 소속되어야 하며, 컨슈머 그룹은 1개 이상의 컨슈머를 가질 수 있다. 
**파티션의 레코드들은 단 하나의 컨슈머에만 할당된다.**

**컨슈머 그룹 내에 1개의 컨슈머만 있을 경우**

<img width="532" alt="image" src="https://github.tossbank.it/storage/user/490/files/74ed0c54-22bb-4f95-8be4-e5239eed6c8e">

- 카프카의 병렬도를 높이는 가장 근본적인 부분은 파티션의 개수
- 단, 파티션의 데이터를 실질적으로 처리하는 대상은 컨슈머이기 때문에 파티션의 개수만큼 컨슈머의 개수를 맞춤으로써 최대의 병렬도를 맞출 수 있다.

**컨슈머 그룹 내에 2개의 컨슈머가 있지만 토픽 파티션 개수보다 작을 경우**

<img width="515" alt="image" src="https://github.tossbank.it/storage/user/490/files/0bdfaa92-cd29-45c3-b785-01fdc3f24496">

> 컨슈머 그룹 내에 컨슈머 변화가 있을 때 마다 파티션과 컨슈머의 조합을 변경하는 **Rebalancing**이 발생한다.

**컨슈머 그룹 내에 파티션 개수와 동일한 컨슈머가 있을 경우**

<img width="505" alt="image" src="https://github.tossbank.it/storage/user/490/files/9dc86d92-6a2c-4792-b518-46d3324e2c4c">

**컨슈머 그룹 내에 파티션 개수보다 많은 컨슈머가 있을 경우**

<img width="524" alt="image" src="https://github.tossbank.it/storage/user/490/files/e9119db9-769e-4197-8e93-d947de51f7a5">

<img width="509" alt="image" src="https://github.tossbank.it/storage/user/490/files/277507de-3902-4128-94a6-b5e8c2056ba0">

파티션은 하나의 컨슈머에만 할당되기 때문에 파티션 개수보다 많게 컨슈머가 늘어도 리밸런싱은 일어나지만 하나의 컨슈머는 일을 하지 않는다. (어떤 컨슈머에 할당될지는 모름)

**하나의 토픽을 여러 개의 컨슈머 그룹이 subscribe 할 경우**

<img width="512" alt="image" src="https://github.tossbank.it/storage/user/490/files/114d09bc-e436-498a-b9a5-5beb2f0d0987">

컨슈머 그룹이 다르면 이전 컨슈머들과 별개로 파티션에서 데이터를 가져가서 처리할 수 있다.

- 모든 컨슈머들은 고유한 그룹아이디 group.id를 가지는 컨슈머 그룹에 소속되어야 한다.
- 동일한 컨슈머 그룹 내의 컨슈머들은 작업량을 최대한 균등하게 배분
- 서로 다른 컨슈머 그룹의 컨슈머들은 분리되어 독립적으로 동작

### 실습

**파티션 3개 토픽 생성**

```text
./kafka-topics --bootstrap-server localhost:9092 --create --topic multipart-topic --partitions 3
```

**컨슈머 그룹 group_01에 multipart-topic subscribe하는 컨슈머 생성**

```text
./kafka-console-consumer --bootstrap-server localhost:9092 --group group_01 --topic multipart-topic \
--property print.key=true --property print.value=true \
--property print.partition=true
```

```text
[2023-12-16 14:01:37,963] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group group_01 in Empty state. Created a new member id console-consumer-0e05df05-2a42-4246-bb5c-04f1435df33e and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-12-16 14:01:37,967] INFO [GroupCoordinator 0]: Preparing to rebalance group group_01 in state PreparingRebalance with old generation 0 (__consumer_offsets-45) (reason: Adding new member console-consumer-0e05df05-2a42-4246-bb5c-04f1435df33e with group instance id None) (kafka.coordinator.group.GroupCoordinator)
[2023-12-16 14:01:37,971] INFO [GroupCoordinator 0]: Stabilized group group_01 generation 1 (__consumer_offsets-45) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-12-16 14:01:37,977] INFO [GroupCoordinator 0]: Assignment received from leader console-consumer-0e05df05-2a42-4246-bb5c-04f1435df33e for group group_01 for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
```

- Stabilized : 리밸런스가 안정화됨을 알린다.

> 하나만 생성되어도 리밸런스는 수행된다.

**해당 그룹에 컨슈머 하나 더 생성**

```text
./kafka-console-consumer --bootstrap-server localhost:9092 --group group_01 --topic multipart-topic \
--property print.key=true --property print.value=true \
--property print.partition=true
```

```text
[2023-12-16 14:07:13,621] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group group_01 in Stable state. Created a new member id console-consumer-17b49ddb-8ad3-46c1-a4be-bdc69c15df6f and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-12-16 14:07:13,623] INFO [GroupCoordinator 0]: Preparing to rebalance group group_01 in state PreparingRebalance with old generation 1 (__consumer_offsets-45) (reason: Adding new member console-consumer-17b49ddb-8ad3-46c1-a4be-bdc69c15df6f with group instance id None) (kafka.coordinator.group.GroupCoordinator)
[2023-12-16 14:07:14,114] INFO [GroupCoordinator 0]: Stabilized group group_01 generation 2 (__consumer_offsets-45) with 2 members (kafka.coordinator.group.GroupCoordinator)
[2023-12-16 14:07:14,116] INFO [GroupCoordinator 0]: Assignment received from leader console-consumer-0e05df05-2a42-4246-bb5c-04f1435df33e for group group_01 for generation 2. The group has 2 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
```

- 해당 그룹에 2개의 컨슈머 존재 (with 2 members)

**해당 그룹 내의 컨슈머 종료**

```text
[2023-12-16 14:10:07,479] INFO [GroupCoordinator 0]: Preparing to rebalance group group_01 in state PreparingRebalance with old generation 4 (__consumer_offsets-45) (reason: Removing member console-consumer-17b49ddb-8ad3-46c1-a4be-bdc69c15df6f on LeaveGroup) (kafka.coordinator.group.GroupCoordinator)
[2023-12-16 14:10:07,479] INFO [GroupCoordinator 0]: Member MemberMetadata(memberId=console-consumer-17b49ddb-8ad3-46c1-a4be-bdc69c15df6f, groupInstanceId=None, clientId=console-consumer, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group group_01 through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator)
[2023-12-16 14:10:08,228] INFO [GroupCoordinator 0]: Stabilized group group_01 generation 5 (__consumer_offsets-45) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-12-16 14:10:08,230] INFO [GroupCoordinator 0]: Assignment received from leader console-consumer-0e05df05-2a42-4246-bb5c-04f1435df33e for group group_01 for generation 5. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
```

- 리밸런싱이 일어나고 총 멤버가 1개

## kafka-consumer-groups 명령어로 Consumer Group과 Consumer, Lag 정보 확인하기

### Consumer Group list 정보

```text
./kafka-consumer-groups --bootstrap-server localhost:9092 --list
```

### Consumer Group과 Consumer 관계, Partition 등에 대한 상세 정보

```text
./kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group group_01
```

```text
GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                           HOST            CLIENT-ID
group_01        multipart-topic 0          0               0               0               console-consumer-989c1546-c1ef-424a-9a02-f19c17a9d983 /127.0.0.1      console-consumer
group_01        multipart-topic 1          0               0               0               console-consumer-989c1546-c1ef-424a-9a02-f19c17a9d983 /127.0.0.1      console-consumer
group_01        multipart-topic 2          0               0               0               console-consumer-989c1546-c1ef-424a-9a02-f19c17a9d983 /127.0.0.1      console-consumer
```

- 현재 컨슈머 1개만 생성되어 해당 토픽의 모든 파티션에 동일한 컨슈머가 할당
- client id 는 이후에 애플리케이션 레벨에서 지정할 수 있다.

컨슈머 1개 추가한 뒤 명령어 재실행

```text
GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                           HOST            CLIENT-ID
group_01        multipart-topic 0          0               0               0               console-consumer-16c18481-4cb1-45a2-b293-9b563744a095 /127.0.0.1      console-consumer
group_01        multipart-topic 1          0               0               0               console-consumer-16c18481-4cb1-45a2-b293-9b563744a095 /127.0.0.1      console-consumer
group_01        multipart-topic 2          0               0               0               console-consumer-989c1546-c1ef-424a-9a02-f19c17a9d983 /127.0.0.1      console-consumer
```

- 리밸런싱이 발생하여 파티션이 재분배된 것을 확인할 수 있음
- 하나의 컨슈머를 더 생성하면 각 파티션은 하나의 컨슈머에 할당되는것을 확인 할 수 있다.

**컨슈머를 모두 종료하고 메시지 producing후 상세정보 확인**

```text
Consumer group 'group_01' has no active members.

GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID
group_01        multipart-topic 0          0               1               1               -               -               -
group_01        multipart-topic 1          0               1               1               -               -               -
group_01        multipart-topic 2          0               2               2               -               -               -
```

- 각 파티션에 메시지가 쌓여있고 LAG이 생긴 것을 확인할 수 있다.
- 이후에 컨슈머를 다시 실행하면 CURRENT-OFFSET과 LOG-END-OFFSET이 같아지고 LAG이 사라진다.

### Consumer Group 삭제

Consumer Group은 그룹 내의 컨슈머가 모두 종료되어도 삭제되지 않는다.(기본 옵션으로 일주일동안 더이상 컨슈머가 소속되지 않으면 삭제)

```text
./kafka-consumer-groups --bootstrap-server localhost:9092 --delete --group group_01
```

- 컨슈머가 하나라도 존재하면 삭제에 실패한다.

## 카프카 환경 파라미터의 구분 및 kafka-config 명령어로 파라미터 검색 및 수정 적용하기

<img width="768" alt="image" src="https://github.tossbank.it/storage/user/490/files/bfea70d0-8fa5-4a11-add4-6016de4a8d82">

- 브로커 레벨에서 설정한 config 설정을 각 토픽에서는 default로 따르게 되고, 토픽 레벨에서 config를 설정하면 해당 설정을 따르게된다.

### Kafka-configs 사용 

<img width="806" alt="image" src="https://github.tossbank.it/storage/user/490/files/32013ed0-9556-4c6c-9064-9d8098eeb7b9">

- --delete 는 이전 디폴트 값으로 변경하는 것이다. (삭제X)

## kafka-dump-log 명령어로 로그 파일의 메시지 내용 확인

프로듀싱 했는데 브로커에 정상적으로 제대로 적재되었는지 등을 파악할 수 있다.

**토픽 삭제 후 재생성**

default로 /tmp/kakfa-logs 폴더에 각 토픽에 대한 파티션 개수만큼(-0, -1, -2) 폴더가 존재하고 해당 폴더 내에 `*.log`에 데이터가 담긴다. \
kafka-dump-log 명령어는 해당 파일을 읽는다. 

```text
./kafka-dump-log --deep-iteration --files /tmp/kafka-logs/multipart-topic-0/00000000000000000000.log --print-data-log
Dumping /tmp/kafka-logs/multipart-topic-0/00000000000000000000.log
```

```text
Starting offset: 0
baseOffset: 0 lastOffset: 296 count: 297 baseSequence: 0 lastSequence: 296 producerId: 4 producerEpoch: 0 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 0 CreateTime: 1702706712174 size: 16332 magic: 2 compresscodec: none crc: 3238154558 isvalid: true
| offset: 0 CreateTime: 1702706712169 keySize: -1 valueSize: 47 sequence: 0 headerKeys: [] payload: test nonkey message sent test00000000000000 300
| offset: 1 CreateTime: 1702706712169 keySize: -1 valueSize: 47 sequence: 1 headerKeys: [] payload: test nonkey message sent test00000000000000 301
| offset: 2 CreateTime: 1702706712169 keySize: -1 valueSize: 47 sequence: 2 headerKeys: [] payload: test nonkey message sent test00000000000000 302
| offset: 3 CreateTime: 1702706712169 keySize: -1 valueSize: 47 sequence: 3 headerKeys: [] payload: test nonkey message sent test00000000000000 303
| offset: 4 CreateTime: 1702706712169 keySize: -1 valueSize: 47 sequence: 4 headerKeys: [] payload: test nonkey message sent test00000000000000 304
| offset: 5 CreateTime: 1702706712169 keySize: -1 valueSize: 47 sequence: 5 headerKeys: [] payload: test nonkey message sent test00000000000000 305
| offset: 6 CreateTime: 1702706712169 keySize: -1 valueSize: 47 sequence: 6 headerKeys: [] payload: test nonkey message sent test00000000000000 306
| offset: 7 CreateTime: 1702706712169 keySize: -1 valueSize: 47 sequence: 7 headerKeys: [] payload: test nonkey message sent test00000000000000 307

...
```

### subscribe, poll, commit 로직

<img width="737" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/77662d99-8538-407c-9621-a496a4d4e575">

<img width="751" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/0466527d-81d4-4757-8ed0-ce4d99e79bf6">

- Consumer는 subscribe를 호출하여 읽어 들이려는 토픽을 등록
- Consumer는 poll() 메소드를 이용하여 주기적으로 브로커의 토픽 파티션에서 메시지를 가져온다.
- 메시지를 성공적으로 가져왔으면 commit을 통해서 __consumer_offset 이라는 인터널 토픽에 의해 다음에 읽을 offset 위치를 기재

### KafkaConsumer의 주요 수행 개요

<img width="645" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/4aac7a54-acdf-4c8d-a07f-fa865f76ac00">

- KafkaConsumer는 Fetcher, ConsumerClientNetwork 등의 주요 내부 객체와 별도의 Heart Beat Thread를 생성
  - Heart Beat Thread를 통해서 컨슈머가 정상 상태임을 브로커에게 지속적으로 알린다.
  - Heart Beat Thread는 Consumer의 정상적인 활동을 Group Coordinator에 보고하는 역할을 수행 -> Group Coordinator는 주어진 시간동안 Heart Beat을 받지 못하면 Consumer들의 리밸런스를 수행하도록 명령한다
- Fetcher, ConsumerClientNetwork 객체는 Broker의 토픽 파티션에서 메시지를 Fetch 및 Poll 수행한다.

> 컨슈머의 close()를 명시적으로 수행해야지만 브로커에서 리밸런싱이 이뤄지기 때문에 중요하다
> 
> 애플리케이션을 강제로 종료하면 해당 컨슈머의 Heart Beat Thread에서 더이상 Heart Beat을 보내지 않기 때문에 브로커의 코디네이터에서 리밸런싱을 진행한다.

## KafkaConsumer 클래스의 주요 구성 요소와 poll() 메서드 동작 매커니즘의 이해 

<img width="228" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/c6f166a9-0481-46aa-972f-41d157f949ab">

```kotlin
comuser.poll(Duration.ofMillis(1000))
```

- Fetcher, ConsumerNetworkClient가 협력하여 브로커로부터 데이터를 Fetch하는 역할을 한다.
- 브로커나 Consumer 내부 큐에 데이터가 있다면 바로 데이터를 반환한다.
- 그렇지 않을 경우에는 1000ms동안 데이터 Fetch를 브로커에 계속 수생하고 결과를 반환한다.

### poll() 메서드의 동작

**poll(Duration.ofMillis(1000)) 수행**

<img width="594" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/ff1612fd-ff10-4994-b136-fefa7035b682">

- Linked Queue에 데이터가 있을 경우 Fetcher는 데이터를 가져오고 반환하며 poll() 수행 완료
- ConsumerNetworkClient는 비동기로 계속 브로커의 메시지를 가져와서 Linked Queue에 저장
- Linked Queue에 데이터가 없을 경우 ConsumerNetworkClient가 1000ms 까지 브로커에 메시지 요청후 poll() 수행 완료

즉 실질적으로 브로커로부터 메시지를 가져오는 역할은 ConsumerNetworkClient가 비동기로 처리하고, Fetcher는 ConsumerNetworkClient가 가져와서 쌓아둔 Linked Queue만을 바라보고 있다

만약에 2번째 poll()을 수행했는데, 브로커에 쌓인 데이터가 없는 경우에 ConsumerNetworkClient는 최대 1초동안 기다리게된다. 

> 실질적으로 첫번째 poll()에서는 데이터를 가져오지 않고 브로커와 메타데이터 교환, HeartBeatThread 생성 등의 작업을 하게된다. 

<img width="718" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/6d29a894-bcc3-4a79-a8b9-52b651f15b9e">

## Consumer Fetcher 관련 주요 파라미터와 Fetcher 매커니즘의 이해

### Consumer Fetcher 관련 주요 파라미터

- fetch.min.bytes
  - Fetcher가 record들을 읽어들이는 최소 bytes. 브로커는 지정된 fetch.min.bytes 이상의 새로운 메시지가 쌓일때 까지 전송하지 않음. (기본은 1byte)
  - 정확히는 Fetcher가 ConsumerNetworkClient에게 메시지를 읽어오기를 요청하는 최대 크기
  - 프로듀서의 batch.size와 비슷한 개념(?)
- fetch.max.wait.ms
  - 브로커에 fetch.min.bytes 이상의 메시지가 쌓일 때까지 최대 대기 시간 (기본은 500ms)
  - ConsumerNetworkClient가 브로커에 fetch.min.bytes까지 쌓일 때까지 무한정 대기하는게 아닌 해당 시간만큼 대기해도 쌓이지 않으면 가져온다.
  - 만약 fetch.max.wait.ms이 되기전에 fetch.min.bytes만큼 데이터가 쌓이면 바로 반환

> poll(millis)은 메시지가 없으면 해당 시간만큼 기다리는 것이고, fetch.max.wait.mx는 메시지가 존재하더라도 기다린다.

- fetch.max.bytes
  - Fetcher가 한 번에 가져올 수 있는 최대 데이터 bytes (기본은 50MB)
- max.partiton.fetch.bytes
  - Fetcher가 파티션별로 한 번에 최대로 가져올 수 있는 bytes (기본은 1MB)
  - 만약 파티션이 10개가 존재한다면 최대 파티션별로 기본 설정 기준 10MB 가져오는것
  - 만약 100개가 존재한다면 총 100MB를 가져오는것이지만 fetch.max.bytes에 의해 제약이 걸린다.
- max.poll.records
  - Fetcher가 한번에 가져올 수 있는 레코드 수(기본은 500)

**poll(1000ms) 수행 정리**

- 가져올 데이터가 1건도 없으면 poll()에 인자 시간만큼 대기 후 Return
- 가져와야할 과거 데이터가 많을 경우 max.partition.fetch.bytes로 파티션별로 배치 크기 설정.
  - 그렇지 않을 경우 fetch.min.bytes로 배치 크기 설정
  - 실질적으로 카프카 문서에 정의된 내용은 아니다. 사실 배치 단위로 잘 가져온다고 하더라도 뒷로직(DB insert ,...)에서 퍼포먼스가 떨어진다.
- 가장 최신의 offset 데이터를 가져오고 있다면 fetch.max.wait.ms 만큼 기다린 후 return
- 오랜 과거 offset 데이터를 가져 온다면 최대 max.partition.fetch.bytes 만큼 파티션에서 읽은 뒤 반환
- max.partition.fetch.bytes에 도달하지 못하여도 가장 최신의 offset에 도달하면 반환
- 토픽에 파티션이 많아도 가져오는 데이터량은 fetch.max.bytes로 제한
- Fetcher가 Linked Queue에서 가져오는 레코드의 개수는 max.poll.records로 제한

```kotlin
val props: MutableMap<String, Any> = HashMap()
props[ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG] = 300000
props[ConsumerConfig.MAX_POLL_RECORDS_CONFIG] = maxPollRecord
props[ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG] = 305000
props[ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG] = 10000
props[ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG] = 1000
props[ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG] = true
props[ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG] = 500
props[ConsumerConfig.AUTO_OFFSET_RESET_CONFIG] = "latest"
props[ConsumerConfig.FETCH_MAX_BYTES_CONFIG] = ConsumerConfig.DEFAULT_FETCH_MAX_BYTES
props[ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG] = 500
props[ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG] = bootstrapServers
props[ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG] = listOf(RangeAssignor::class.java)
return props
```

<img width="450" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/60e3a7b3-8058-445c-b94b-233bc1c1dff0">

- minBytes(fetch.min.bytes): 배치 사이즈(?)
- fetchSize(max.partiton.fetch.bytes): 가져올 데이터가 많다면 해당 크기만큼 늘어날 수 있다.

## __consumer_offsets 내부 토픽(internal topic) 뜯어보기

기본적으로 __consumer_offsets은 인터널 토픽으로 컨슈머로 읽기가 허용되지 않는다. (exclude.internal.topics=true)

해당 옵션을 바꿔서 컨슈머를 통해 읽도록 할 수 있다.

```text
1. consumer.config용 config 파일을 생성. 
echo "exclude.internal.topics=false" > consumer_temp.config

2. __consumer_offsets 토픽을 읽기
kafka-console-consumer --consumer.config /Users/dudwls143/kafka-practice/confluent-7.1.2/consumer_temp.config \
 --bootstrap-server localhost:9092 --topic __consumer_offsets \
 --formatter "kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter"
```

**컨슈머 구동 시 결과 (메시지 3개 프로듀싱한 상태)**

```text
[group_01,simple-topic,0]::OffsetAndMetadata(offset=3, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1703405688004, expireTimestamp=None)
[group_01,simple-topic,0]::OffsetAndMetadata(offset=3, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1703405693017, expireTimestamp=None)
[group_01,simple-topic,0]::OffsetAndMetadata(offset=3, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1703405698011, expireTimestamp=None)
[group_01,simple-topic,0]::OffsetAndMetadata(offset=3, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1703405703010, expireTimestamp=None)
[group_01,simple-topic,0]::OffsetAndMetadata(offset=3, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1703405708012, expireTimestamp=None)
[group_01,simple-topic,0]::OffsetAndMetadata(offset=3, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1703405713017, expireTimestamp=None)
[group_01,simple-topic,0]::OffsetAndMetadata(offset=3, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1703405718015, expireTimestamp=None)
```

**현재 상태에서 데이터 하나를 더 send**

해당 record의 offset를 출력 시 3(record 자신의 offset)를 출력하지만 __consumer_offset의 offset은 4(다음 읽을 offset 번호)를 출력한다.

```text
[main] INFO ConsumerWakeUp - record key:null, record value:d, partition:0, record offset:3
```

```text
[group_01,simple-topic,0]::OffsetAndMetadata(offset=4, leaderEpoch=Optional[0], metadata=, commitTimestamp=1703405873051, expireTimestamp=None)
```

## auto.offset.reset의 내부 동작 매커니즘

이전에 알고있던 해당 파라미터에 대한 개념은 Consumer가 토픽에 처음 접속하여 메시지를 가져올 때 처음 offset(earliest)부터 가져올 것인지, 가장 마지막 offset 이후부터 가져올 것인지를 결정하는 파라미터로 알고있었다.

이를 좀 더 상세히 설명하면

- 동일 Consumer Group 으로 Consumer가 새롭게 접속할 시 __consumer_offset에 있는 offset 정보를 기반으로 메시지를 가져오기 때문에 earliest로 설정하여도 0번 오프셋부터 읽어드리진 않는다.
- Consumer Group의 Consumer가 모두 종료되어도 Consumer Group이 읽어들인 offset 정보는 7일동안 __consumer_offset에 저장되어 있다. (offsets.retention.minutes)
- 해당 토픽이 삭제되고 재생성될 경우에는 해당 토픽에 대한 Consumer Group의 offset 정보는 0으로 __consumer_offset으로 기록된다.

> __consumer_offset은 컨슈머 그룹별로 토픽의 개별 파티션별로 오프셋 정보를 가지고있다.

```kotlin
props[ConsumerConfig.AUTO_OFFSET_RESET_CONFIG] = "earliest"
```

해당 옵션은 __consumer_offset에 값이 없을 때 결정할 수 있는 옵션이다.
만약 이미 해당 토픽의 개별 파티션에 오프셋 정보를 가지고 있다면 영향을 끼칠 수 없다. (즉, 해당 옵션은 __consumer_offset에 해당 토픽의 개별 파티션에 대한 값이 없을 경우에만 영향을 끼칠 수 있다)

토픽을 지우고 토픽을 다시 생성하면 [group_01,simple-topic,0] 해당 값이 초기화되므로 해당 옵션을 사용할 수 있다. (프로듀싱도 다시해야함)

> 참고로 이전에 ./kafka-console-consumer의 --from-beginning을 하게될 경우에는 항상 새로운 그룹을 만들기 때문에 처음부터 읽어들일 수 있다.

## Group Coordinator와 Consumer의 Rebalance 상세 매커니즘 이해

컨슈머 그룹 내에 새로운 컨슈머가 추가되거나 기존 컨슈머가 종료 될 때, 또는 토픽에 새로운 파티션이 추가될 때, Heart Beat Thread로부터 Heart Beat이 안올 때, 브로커의 Group Coordinator는 컨슈머 그룹내의 컨슈머들에게 파티션을 재할당하는 리밸런싱을 수행하도록 리더 컨슈머(그룹내의 리더)에게 지시한다.

<img width="574" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/5a0873f0-a2c1-47de-8c53-bd692f946efc">

> 컨슈머가 리밸런싱을 시도할 때 메시지 읽기를 잠시 멈추고 리밸런싱에 집중한 뒤 다시 메시지를 읽게 되는데, 이때 안정성이 떨어질 수 있다

### Group Coordinator

- Consumer들의 Join Group 정보, Partition 매핑 정보 관리
- Consumer들의 HearBeat 관리

**컨슈머 그룹내에 새로운 컨슈머가 생성**

<img width="592" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/9634efbc-c8a9-4dfd-b1bf-4f5b1ff74f19">

1. 컨슈머 그룹내의 컨슈머가 브로커에 최초 접속 요청 시 Group Coordinator 생성
2. 동일 group.id로 여러 개의 컨슈머로 브로커의 Group Coordinator로 접속
3. 가장 빨리 그룹에 조인 요청을 한 컨슈머에게 컨슈머 그룹 내의 리더 컨슈머로 지정
4. 리더로 지정된 컨슈머는 파티션 할당전략에 따라 컨슈머들에게 파티션 할당
5. 리더 컨슈머는 최종 할당된 파티션 정보를 Group Coordinator에게 전달
6. 정보 전달 성공을 공유한 뒤 개별 컨슈머들은 할당된 파티션에게 메시지 읽음

**Consumer Group Status**

컨슈머 그룹은 GroupMetadata에 Consumer Group Status를 관리한다. (empty, rebalance, stable)

<img width="702" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/b440babf-7776-442d-9e04-bdd6ce96db8a">

### Consumer 1개 새롭게 할당

**컨슈머 메시지**

```text
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-group_01-1, groupId=group_01] Discovered group coordinator localhost:9092 (id: 2147483647 rack: null)
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-group_01-1, groupId=group_01] (Re-)joining group
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-group_01-1, groupId=group_01] Request joining group due to: need to re-join with the given member-id: consumer-group_01-1-6d52d392-dc3c-4703-ac2f-345c1c595c3f
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-group_01-1, groupId=group_01] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-group_01-1, groupId=group_01] (Re-)joining group
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-group_01-1, groupId=group_01] Successfully joined group with generation Generation{generationId=18, memberId='consumer-group_01-1-6d52d392-dc3c-4703-ac2f-345c1c595c3f', protocol='range'}
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-group_01-1, groupId=group_01] Finished assignment for group at generation 18: {consumer-group_01-1-6d52d392-dc3c-4703-ac2f-345c1c595c3f=Assignment(partitions=[pizza-topic-0, pizza-topic-1, pizza-topic-2])}
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-group_01-1, groupId=group_01] Successfully synced group in generation Generation{generationId=18, memberId='consumer-group_01-1-6d52d392-dc3c-4703-ac2f-345c1c595c3f', protocol='range'}
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-group_01-1, groupId=group_01] Notifying assignor about the new Assignment(partitions=[pizza-topic-0, pizza-topic-1, pizza-topic-2])
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-group_01-1, groupId=group_01] Adding newly assigned partitions: pizza-topic-0, pizza-topic-1, pizza-topic-2
```

- 최초 컨슈머이므로 리더 컨슈머로 할당
- `Adding newly assigned partitions: pizza-topic-0, pizza-topic-1, pizza-topic-2`: 리더 컨슈머이므로 직접 파티션을 할당

**브로커 메시지**

```text
[2023-12-25 15:27:55,540] INFO [GroupCoordinator 0]: Dynamic member with unknown member id joins group group_01 in Empty state. Created a new member id consumer-group_01-1-6d52d392-dc3c-4703-ac2f-345c1c595c3f and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-12-25 15:27:55,542] INFO [GroupCoordinator 0]: Preparing to rebalance group group_01 in state PreparingRebalance with old generation 17 (__consumer_offsets-45) (reason: Adding new member consumer-group_01-1-6d52d392-dc3c-4703-ac2f-345c1c595c3f with group instance id None) (kafka.coordinator.group.GroupCoordinator)
[2023-12-25 15:27:55,543] INFO [GroupCoordinator 0]: Stabilized group group_01 generation 18 (__consumer_offsets-45) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-12-25 15:27:55,552] INFO [GroupCoordinator 0]: Assignment received from leader consumer-group_01-1-6d52d392-dc3c-4703-ac2f-345c1c595c3f for group group_01 for generation 18. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
```

- `Dynamic member with unknown member id joins group group_01 in Empty state`: 컨슈머 그룹내에 컨슈머가 존재하지 않았기 때문에 empty state, join 메시지
- `new member id consumer-group_01-1-6d52d392-dc3c-4703-ac2f-345c1c595c3f`: 그룹 코디네이터가 해당 컨슈머에 `consumer-group_01-1-6d52d392-dc3c-4703-ac2f-345c1c595c3f` 라는 아이디 할당
- `Assignment received from leader`: 리더가 파티션을 정상적으로 할당했음을 확인

## Consumer 스태틱 그룹 멤버쉽(Static Group Membership)의 이해

### 스태틱 그룹 멤버쉽의 필요성

- 많은 컨슈머를 가지는 컨슈머 그룹에서 리밸런스가 발생하면 모든 컨슈머들이 리밸런스를 수행하므로 많은 시간이 소모되고 대량 데이터 처리시 Lag가 더 길어질 수 있다
- 유지보수 차원의 컨슈머 Restart도 리밸런스를 초래하므로 불필요한 리밸런스를 발생시키지 않을 방법이 대두
  - 특정 컨슈머 클라이언트를 특정 원인 파악을 위해 잠깐 내리는 경우에도 해당 컨슈머가 포함된 컨슈머 그룹내의 컨슈머들도 모두 리밸런스 수행

**스태틱 그룹 멤버쉽**

- 컨슈머 그룹내의 컨슈머들에게 고정된 id를 부여
- 컨슈머 별로 컨슈머 그룹 최초 조인 시 할당된 파티션을 그대로 유지하고 컨슈머가 shutdown되어도 session.timeout.ms(디폴트 45초)내에 재기동되면 리밸런스가 수행되지 않고, 기존 파티션이 재할당된다.

<img width="323" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/4baa4d85-6d07-48ca-b382-1f5e68f1cc64">

- 컨슈머 3이 종료되었지만 리밸런스가 일어나지 않으며, 파티션 3은 다른 컨슈머에 재할당되지 않고 읽혀지지 않는다.
- 컨슈머 3이 session.timeout.ms 내에 다시 기동되면 파티션 3은 컨슈머 3에 할당
- 컨슈머 3이 session.timeout.ms 내에 기동되지 않으면 리밸런스가 수행된 후 파티션 3이 다른 컨슈머에 할당

일반적으로 스태틱 그룹 멤버쉽을 적용할 경우 session.timeout.ms를 디폴트 값(45초)가 아닌 더 큰 값으로 설정한다.

**실습 - group.instance.id 할당한 뒤 컨슈머 3개 기동**

```kotlin
props[ConsumerConfig.GROUP_INSTANCE_ID_CONFIG] = "1"
```

```text
[2023-12-25 15:56:01,548] INFO [GroupCoordinator 0]: Static member with groupInstanceId=1 and unknown member id joins group group-01-static in Empty state. Created a new member id 1-d436783c-67ed-4fc3-89c0-85ce41f21523 for this member and add to the group. (kafka.coordinator.group.GroupCoordinator)
[2023-12-25 15:56:01,548] INFO [GroupCoordinator 0]: Preparing to rebalance group group-01-static in state PreparingRebalance with old generation 0 (__consumer_offsets-46) (reason: Adding new member 1-d436783c-67ed-4fc3-89c0-85ce41f21523 with group instance id Some(1)) (kafka.coordinator.group.GroupCoordinator)
[2023-12-25 15:56:01,549] INFO [GroupCoordinator 0]: Stabilized group group-01-static generation 1 (__consumer_offsets-46) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-12-25 15:56:01,553] INFO [GroupCoordinator 0]: Assignment received from leader 1-d436783c-67ed-4fc3-89c0-85ce41f21523 for group group-01-static for generation 1. The group has 1 members, 1 of which are static. (kafka.coordinator.group.GroupCoordinator)

[2023-12-25 15:58:22,875] INFO [GroupCoordinator 0]: Static member with groupInstanceId=2 and unknown member id joins group group-01-static in Stable state. Created a new member id 2-6d64167e-6e7d-4994-9889-5f8cf41f81f5 for this member and add to the group. (kafka.coordinator.group.GroupCoordinator)
[2023-12-25 15:58:22,875] INFO [GroupCoordinator 0]: Preparing to rebalance group group-01-static in state PreparingRebalance with old generation 1 (__consumer_offsets-46) (reason: Adding new member 2-6d64167e-6e7d-4994-9889-5f8cf41f81f5 with group instance id Some(2)) (kafka.coordinator.group.GroupCoordinator)
[2023-12-25 15:58:25,635] INFO [GroupCoordinator 0]: Stabilized group group-01-static generation 2 (__consumer_offsets-46) with 2 members (kafka.coordinator.group.GroupCoordinator)
[2023-12-25 15:58:25,640] INFO [GroupCoordinator 0]: Assignment received from leader 1-d436783c-67ed-4fc3-89c0-85ce41f21523 for group group-01-static for generation 2. The group has 2 members, 2 of which are static. (kafka.coordinator.group.GroupCoordinator)

[2023-12-25 16:00:04,137] INFO [GroupCoordinator 0]: Static member with groupInstanceId=3 and unknown member id joins group group-01-static in Stable state. Created a new member id 3-5b5c1769-ef2c-42de-a31a-f80b7d2c0722 for this member and add to the group. (kafka.coordinator.group.GroupCoordinator)
[2023-12-25 16:00:04,140] INFO [GroupCoordinator 0]: Preparing to rebalance group group-01-static in state PreparingRebalance with old generation 2 (__consumer_offsets-46) (reason: Adding new member 3-5b5c1769-ef2c-42de-a31a-f80b7d2c0722 with group instance id Some(3)) (kafka.coordinator.group.GroupCoordinator)
[2023-12-25 16:00:04,701] INFO [GroupCoordinator 0]: Stabilized group group-01-static generation 3 (__consumer_offsets-46) with 3 members (kafka.coordinator.group.GroupCoordinator)
[2023-12-25 16:00:04,706] INFO [GroupCoordinator 0]: Assignment received from leader 1-d436783c-67ed-4fc3-89c0-85ce41f21523 for group group-01-static for generation 3. The group has 3 members, 3 of which are static. (kafka.coordinator.group.GroupCoordinator)
```

- `1 of which are static.`: 스태픽 그룹 멤버쉽 적용 확인

```text
./kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group group-01-static
```

```text
GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                            HOST            CLIENT-ID
group-01-static pizza-topic     0          0               0               0               1-d436783c-67ed-4fc3-89c0-85ce41f21523 /127.0.0.1      consumer-group-01-static-1
group-01-static pizza-topic     1          0               0               0               2-6d64167e-6e7d-4994-9889-5f8cf41f81f5 /127.0.0.1      consumer-group-01-static-2
group-01-static pizza-topic     2          0               0               0               3-5b5c1769-ef2c-42de-a31a-f80b7d2c0722 /127.0.0.1      consumer-group-01-static-3
```

- CONSUMER-ID: 브로커가 각 컨슈머에 할당한 고유 아이디 - 스태틱 멤버쉽과 관련X
- CLIENT-ID: consumer-group-01-static-1, 해당 id에서 1, 2, 3이 group.instance.id

**현재 상황에서 45초내에 컨슈머 종료 후 재기동하면 리밸런스가 이뤄지지 않는다.**

```text
[2023-12-25 16:06:19,144] INFO [GroupCoordinator 0]: Static member with groupInstanceId=3 and unknown member id joins group group-01-static in Stable state. Replacing previously mapped member 3-5b5c1769-ef2c-42de-a31a-f80b7d2c0722 with this groupInstanceId. (kafka.coordinator.group.GroupCoordinator)
[2023-12-25 16:06:19,149] INFO [GroupCoordinator 0]: Static member which joins during Stable stage and doesn't affect selectProtocol will not trigger rebalance. (kafka.coordinator.group.GroupCoordinator)
[2023-12-25 16:06:19,833] INFO [GroupCoordinator 0]: Request memberId=3-5b5c1769-ef2c-42de-a31a-f80b7d2c0722 for static member with groupInstanceId=3 is fenced by current memberId=3-54d5686a-a632-4dd3-800c-3d1aed53a554 during operation offset-commit (kafka.coordinator.group.GroupCoordinator)
```

## HearBeat 쓰레드와 관련 주요 파라미터들의 이해

### HearBeat 쓰레드 

> 최초 poll() 시도할 때 HearBeat 쓰레드가 생성

HearBeat 쓰레드를 통해서 브로커의 Group Coordinator에 컨슈머 상태를 전송한다.

### Heart beat과 poll() 관련 주요 컨슈머 파라미터

- heartbeat.interval.ms
  - HeartBeat 쓰레드가 Heart Beat을 보내는 간격
  - session.timeout.ms 보다 낮게 설정되어야 한다. (1/3정도를 권장)
  - 기본값: 3000ms
- session.timeout.ms
  - 브로커가 컨슈머로 HeartBeat을 기다리는 최대시간
  - 브로커는 해당 시간동안 HeartBeat을 컨슈머로 부터 받지 못하면 컨슈머를 그룹에서 제외하도록 리밸런싱을 지시
  - 기본값: 45000ms
- max.poll.interval.ms
  - 이전 poll() 호출 후 다음 호출 poll()까지 브로커가 기다리는 시간
  - 해당 시간동안 poll() 호출이 컨슈머로부터 이뤄지지 않으면 해당 컨슈머느 문제가 있는 것으로 판단하고 브로커는 리밸런싱 지시
  - 기본값: 300000ms

<img width="789" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/f5163df9-b2e4-4bbc-961d-97a225ee56a8">

## Consumer에 Rebalance의 Eager 모드와 Cooperative 모드 이해

### Eager 모드

> 디폴트 모드

<img width="810" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/a195ec15-9ea8-4e07-838f-e239c4ad972e">

- 리밸런싱 수행 시 기존 컨슈머들의 모든 파티션 할다을 취소하고 잠시 메시지를 읽지 않는다. 이후 새롭게 컨슈머에 파티션을 다시 할당받고 다시 메시지를 읽음
- 모든 컨슈머가 잠시 메시지를 읽지 않는 시간으로 인해 Lag가 상대적으로 크게 발생할 가능성이 존재
- 리밸런스가 한번 수행 (전체 취소 -> 전체 재할당)
- 파티션 할당 전략(partition.assignment.strategy)중 Range, Round Robin, Sticky 방식이 여기에 해당

### (Incremental) Cooperative 모드

<img width="817" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/9ba481f6-9200-4b83-8ed5-b63add259a59">

- 리밸런스 수행 시 기존 컨슈머들의 모든 파티션 할당을 취소하지 않고 대상이 되는 컨슈머들에 대해서 파티션에 따라 점진적으로 컨슈머를 할당하면서 리밸런스를 수행
- 전체 컨슈머가 메시지 읽기를 중지하지 않으며 개별 컨슈머가 협력적으로 영향을 받는 파티션만 리밸런스로 재분배, 많은 컨슈머를 가지는 컨슈머 그룹내에서 리밸런스 시간이 오래 걸릴 시 활용도가 높다
- 리밸런스가 두번 수행 
- 파티션 할당 전략(partition.assignment.strategy)중 Cooperative Sticky 방식이 여기에 해당

## Consumer의 파티션 할당 전략

### Range 할당 전략

- 기본 파티션 할당 전략
- 서로 다른 2개 이상의 토픽을 컨슈머들이 Subscribe 할 시 **토픽별 동일한 파티션을 특정 컨슈머에게 할당**하는 전략
- 여러 토픽들에서 동일한 키값으로 되어 있는 파티션은 특정 컨슈머에 할당하여 해당 컨슈머가 여러 토픽의 동일 키값으로 데이터 처리를 용이하게 할 수 있도록 지원

```text
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-group-assign-1, groupId=group-assign] Request joining group due to: group is already rebalancing
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-group-assign-1, groupId=group-assign] Revoke previously assigned partitions topic-p3-t1-0, topic-p3-t1-1, topic-p3-t1-2, topic-p3-t2-0, topic-p3-t2-1, topic-p3-t2-2
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-group-assign-1, groupId=group-assign] (Re-)joining group
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-group-assign-1, groupId=group-assign] Successfully joined group with generation Generation{generationId=2, memberId='consumer-group-assign-1-04d03ab8-3be8-4e8f-9763-5295883ae095', protocol='range'}
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-group-assign-1, groupId=group-assign] Finished assignment for group at generation 2: {consumer-group-assign-1-17ac8dd7-ed39-4c80-83e7-118ad233f8f8=Assignment(partitions=[topic-p3-t2-2, topic-p3-t1-2]), consumer-group-assign-1-04d03ab8-3be8-4e8f-9763-5295883ae095=Assignment(partitions=[topic-p3-t2-0, topic-p3-t2-1, topic-p3-t1-0, topic-p3-t1-1])}
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-group-assign-1, groupId=group-assign] Successfully synced group in generation Generation{generationId=2, memberId='consumer-group-assign-1-04d03ab8-3be8-4e8f-9763-5295883ae095', protocol='range'}
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-group-assign-1, groupId=group-assign] Notifying assignor about the new Assignment(partitions=[topic-p3-t2-0, topic-p3-t2-1, topic-p3-t1-0, topic-p3-t1-1])
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-group-assign-1, groupId=group-assign] Adding newly assigned partitions: topic-p3-t1-0, topic-p3-t1-1, topic-p3-t2-0, topic-p3-t2-1
```

- `Revoke previously assigned partitions`: Eager 모드이므로 해당 컨슈머에 할당되었던 파티션을 모두 revoke
- `Adding newly assigned partitions: topic-p3-t1-0, topic-p3-t1-1, topic-p3-t2-0, topic-p3-t2-1`: RangeAssignor이기 때문에 다른 토픽의 동일한 파티션이 할당된다.


### Round Robin 할당 전략

- 파티션별로 컨슈머들이 **균등하게 부하를 분배**할 수 있도록 여러 토픽들의 파티션들을 컨슈머들에게 순차적으로 라운드 로빈 방식으로 할당

**Round Robin vs. Range 비교 - Case 1**

<img width="784" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/73f061a9-c892-4801-8961-0ee987bee9db">

- Round Robin은 토픽들의 파티션별로 순차적으로 컨슈머에 할당하므로 파티션 매핑이 컨슈머별로 비교적 균등하게 할당
  - 서로 다른 토픽의 파티션이더라도 순차적으로 컨슈머에 할당 
- Range는 서로 다른 토픽들의 동일한 파티션들을 같은 컨슈머로 할당. 
  - 토픽 A의 파티션 1번이 컨슈머 1에 할당되면 토픽 B의 파티션 2번도 컨슈머 1에 할당되는 전략
  - 서로 다른 토픽에서 동일한 키값을 가지는 파티션들은 같은 컨슈머에서 처리할 수 있도록 유도
  - Round Robin 방식에 비해서 각 컨슈머에 균등하게 파티션이 분배되지 않을 수 있지만 데이터 처리 측면에서 더욱 효과적일 수 있다.

**Round Robin vs. Range 비교 - Case 2**

<img width="800" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/22c0c240-a2d9-42cb-9afe-0d5addb1a4ed">

**Round Robin의 리밸런스 후 파티션, 컨슈머 매핑**

<img width="785" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/fbc69fbb-a449-436e-a537-bfed1e45846e">

- 라운드 로빈의 경우 리밸런스 시에도 토픽들의 파티션과 컨슈머들을 균등하게 매핑하려고 하므로 리밸런스 이전의 파티션과 컨슈머들의 매핑이 변경되기 쉽다.

### Stick 할당 전략

<img width="791" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/e18e92a7-d404-442f-a094-2011950972d3">

- 최초에 할당된 **파티션과 컨슈머 매핑을 리밸런스가 수행되어도 가급적 그대로 유지**할 수 있도록 지원하는 전략
- 하지만 Eager 모드 기반이므로 리밸런스 시 모든 컨슈머의 파티션 매핑이 해제된 후에 다시 매핑되는 형태
  - 이전 매핑 정보를 기억


### Cooperative Sticky 할당 전략

<img width="795" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/e04a679b-99a8-46ce-8c63-6e847ebf4ca0">

- 최초에 할당된 파티션과 컨슈머 매핑을 리밸런스가 수행되어도 가급적 그대로 유지할 수 있도록 지원함과 동시에 Cooperative 전략 기반으로 **리밸런스 시 모든 컨슈머의 파티션 매핑이 해제되지 않고 리밸런스 연관된 파티션과 컨슈머만 재매핑**된다.
- Stick 할당 전략과 달리 모든 파티션, 컨슈머 매핑이 해제되지 않는다.

```text
partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
```

디폴트 옵션은 위와 같이 세팅되어있다.
RangeAssignor를 디폴트로 사용하고, RangeAssignor 전략에 문제가 존재하면 CooperativeStickyAssignor를 활용

**컨슈머 1개 실행**

```text
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-group-assign-1, groupId=group-assign] Updating assignment with
	Assigned partitions:                       [topic-p3-t1-0, topic-p3-t1-1, topic-p3-t1-2, topic-p3-t2-0, topic-p3-t2-1, topic-p3-t2-2]
	Current owned partitions:                  []
	Added partitions (assigned - owned):       [topic-p3-t1-0, topic-p3-t1-1, topic-p3-t1-2, topic-p3-t2-0, topic-p3-t2-1, topic-p3-t2-2]
	Revoked partitions (owned - assigned):     []
```

**컨슈머 2개 실행**

- 기존 컨슈머 첫번째 리밸런스 
```text
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-group-assign-1, groupId=group-assign] Updating assignment with
	Assigned partitions:                       [topic-p3-t1-0, topic-p3-t1-1, topic-p3-t1-2]
	Current owned partitions:                  [topic-p3-t1-0, topic-p3-t1-1, topic-p3-t1-2, topic-p3-t2-0, topic-p3-t2-1, topic-p3-t2-2]
	Added partitions (assigned - owned):       []
	Revoked partitions (owned - assigned):     [topic-p3-t2-0, topic-p3-t2-1, topic-p3-t2-2]
```

- 기존 컨슈머 두번째 리밸런스
```text
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-group-assign-1, groupId=group-assign] Updating assignment with
	Assigned partitions:                       [topic-p3-t1-0, topic-p3-t1-1, topic-p3-t1-2]
	Current owned partitions:                  [topic-p3-t1-0, topic-p3-t1-1, topic-p3-t1-2]
	Added partitions (assigned - owned):       []
	Revoked partitions (owned - assigned):     []
```

- 새로운 컨슈머
```text
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-group-assign-1, groupId=group-assign] Updating assignment with
	Assigned partitions:                       [topic-p3-t2-0, topic-p3-t2-1, topic-p3-t2-2]
	Current owned partitions:                  []
	Added partitions (assigned - owned):       [topic-p3-t2-0, topic-p3-t2-1, topic-p3-t2-2]
	Revoked partitions (owned - assigned):     []
```

## Consumer의 읽기 Offset Commit(커밋)과 중복 읽기 상황의 이해

### Offset Commit의 이해

<img width="757" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/653aa083-5fdb-4938-90de-20e271d2f8df">

**중복 읽기 상황**

<img width="732" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/62e3a260-9dd1-4433-92bd-e89f4f32c49e">

- 컨슈머 1이 5번 오프셋부터 8번까지 읽어들이고, 데이터를 프로세싱해서 DB에 insert까지 수행
- 특정 이슈로 인해서 브로커에 커밋을 수행하지 못하고 컨슈머 1이 종료
- 리밸런싱이 발생하고 컨슈머 2는 커밋되지 않았으므로 5번 오프셋부터 다시 읽고 DB에 insert를 수행 -> 중복 발생

카프카와 같은 분산 프레임워크를 사용할 때 성능을 향상시키기 위해서 이러한 중복 처리가 발생함을 항상 생각해야한다. (성능 vs. 무결성)

**읽기 누락(loss) 상황**

<img width="670" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/8edb0f6d-b504-403e-995b-334e21d2e387">

- poll() 수행하자마자 커밋을 적용하는 케이스
- 중간쯤 데이터를 처리하는 와중에 컨슈머가 종료
- 리밸런싱이 발생하고, 컨슈머 2가 9번 오프셋부터 데이터를 읽어서 처리


## Consumer의 auto commit 이해

- 컨슈머 파라미터로 auto.enable.commit=true 인 경우 읽어온 메시지를 브로커에 바로 커밋 적용하지 않는다.
- auto.commit.interval.ms에 정해진 주기(기본 5초)마다 컨슈머가 자동으로 커밋을 수행한다.
- 컨슈머가 읽어온 메시지보다 브로커의 커밋이 오래 되었으므로 컨슈머의 장애/재기동 및 리밸런싱 후 브로커에서 이미 읽어온 메시지를 다시 읽어와서 중복 처리 될 수 있다.

<img width="667" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/dd52f068-bd25-44a8-a42e-42138b53e10d">

- auto.commit.interval.ms=5(디폴트)
- 1번 poll() 수행 시간이 3초이고, 2번 poll()할 때 6초가 걸리고 3번 offset까지 가져왔다고 가정하자.
- 3번째 poll()을 수행할 때 auto.commit.interval.ms 보다 수행 시간이 지났기 때문에 commit을 시도한다.
  - 물론 데이터도 가져옴
- 이때 __consumer_offset에는 현재 3번까지 가져왔으므로 다음 가져올 offset 4를 기록한다.

**auto commit 적용 시 consumer를 정상적으로 close하게되면 해당 offset까지 커밋을 수행한다.**

단, 비정상적인 종료(wakeup 호출X -> close 호출X)인 경우에는 중복 읽기 상황이 발생할 수 있다. (즉, 비정상적으로 종료된 컨슈머가 poll()을 통해 읽어온 offset이 커밋이 수행되지 않을 수 있다. - 컨슈머에서는 65번 오프셋까지 읽었으나 재실행하면 이전 offset부터 읽어올 수 있다.)

## Consumer의 Manual Commit(수동 커밋) 이해

일정 주기마다 자동으로 커밋(auto commit)하지 않고 API를 이용하여 동기 또는 비동기 Commit 적용이 가능하다.

- enable.auto.commit=false

### 동기(Sync) 방식

- 컨슈머 객체의 commitSync() 메소드 사용
- 메시지 배치를 poll()을 통해서 읽어오고 해당 메시지들의 마지막 offset을 브로커에 커밋
- **브로커에 커밋 적용이 성공적으로 될 때까지 블락**
- 커밋 적용 완료 후에 다시 메시지를 읽음
- **브로커에 커밋 적용이 실패할 경우 다시 커밋 적용 요청**
- 비동기 방식 대비 느린 수행시간
  - 비동기 방식과 비교했을 때 중복 읽기 가능성이 떨어지긴 하지만 가능성이 0은 아님

### 비동기(Async) 방식

- 컨슈머 객체의 commitAsync() 메소드 사용
- 메시지 배치를 poll()을 통해서 읽어오고 해당 메시지들의 마지막 offset을 브로커에 커밋 요청하지만 **브로커에 커밋 적용이 성공적으로 되었음을 기다리지 않고(블락X)** 계속 메시지를 읽음
- **브로커에 커밋 적용이 실패해도 다시 커밋 시도 X** 때문에 컨슈머 장애 또는 리밸런스 시 한번 읽은 메시지를 다시 중복해서 가져올 수 있다.
- 동기 방식 대비 빠른 수행시간

```kotlin
consumer.commitAsync(object : OffsetCommitCallback {
  override fun onComplete(
    offsets: MutableMap<TopicPartition, OffsetAndMetadata>,
    exception: Exception?,
  ) {
    if (exception != null) {
      logger.error("offsets: $offsets is not completed, error: $exception")
    }
  }
})
```

비동기 방식에서는 컨슈머가 종료될 때 커밋을 수행하기 위해 명시적으로 `consumer.commitSync()`를 호출해야한다.

```kotlin
...
finally {
        consumer.commitSync()
        logger.info("finally consumer is closing")
        consumer.close()
    }
```

## 컨슈머에서 토픽의 특정 파티션만 명시적으로 할당

- 컨슈머에게 여러 개의 파티션이 있는 토픽에서 특정 파티션만 할당 가능하다
- ex. 배치 처리시 특정 key레벨의 파티션을 특정 컨슈머에 할당하여 처리할 경우 적용
- assign() 메서드에 TopicPartition 객체로 특정 파티션을 인자로 입력하여 할당

```kotlin
val topicPartition = TopicPartition(topicName, 0)
consumer.assign(topicPartition)
```

## 컨슈머에서 토픽의 특정 파티션의 특정 offset 부터 읽기

<img width="672" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/9fec2dc6-f997-4bf0-ae57-c6c82e46c15c">

- 특정 메시지가 누락되었을 경우 해당 메시지를 다시 읽어 오기 위해 유지 보수 차원에서 일반적으로 사용
- TopicPartition 객체로 할당할 특정 파티션을 설정하고 seek() 메소드로 읽어올 offset 설정

```kotlin
val topicPartition = TopicPartition(topicName, 1)
consumer.assign(topicPartition)
consumer.seek(topicPartition, 6L)
```


----

poll() 인자로 Duration을 넣는데, 해당 인자는 얼만큼 브로커로부터 메시지가 쌓일때까지 대기할지 결정하는 인자이다.
즉, 10초로 설정했다면 10초를 기다려도 브로커에 데이터가 쌓이지 않으면 empty를 반환한다.

poll() 호출 주기는 (레코드 하나를 처리하는데 걸리는 시간) X (poll 메소드를 통해 가져온 레코드의 수)로 결정된다.
즉, max.poll.records를 10으로 설정하고 레코드 하나를 처리하는데 5초가 걸린다면, 50초 주기로 poll()이 호출된다. (매번 10개를 가져오는건 아니다. 가능하면 최대 10개를 가져오는것)

여기서 중요한점은 poll 메서드의 호출 주기를 짧게 유지해야 파티션 리밸런싱이 완료되기까지 걸리는 시간도 짧아진다는 점이다. 

<img width="834" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/97f22a16-df5a-4c5f-a84d-3da84a4ae640">

위와 같은 상황을 가정해보자. (컨슈머 그룹내에 컨슈머 3개 존재, poll 호출 주기가 250초)

1. 컨슈머 A가 close()를 호출하며 정상 종료
2. 컨슈머 그룹에 변화가 생겼기 때문에 파티션 리밸런싱이 일어난다. 이때 만약 컨슈머 B는 컨슈머 A가 그룹에서 제외된 후 얼마 지나지 않아서 poll 메소드를 호출했다면 컨슈머 B는 브로커의 코디네이터에게 poll 메서드를 통해 조인 요청을 보낸다. 그리고 코디네이터가 리더를 선출해서 응답을 줄때까지 기다린다.
3. 컨슈머 C는 이전 poll을 통해 가져온 레코드를 전부 처리하지 못했기 때문에 poll 메서드를 아직 호출하지 못하고 있는 상황이라면 코디네이터는 컨슈머 C가 poll 메서드를 호출하지 못해 조인 요청을 받지 못하고 있기 떼문에 리더를 선출할 수 없으며, 컨슈머 B는 여전히 대기
4. 컨슈머 C가 약 150초 정도 지난 뒤에 poll 호출했다고 가정하면, 이제서야 코디네이터는 그룹 내의 모든 컨슈머로부터 조인 요청을 받았기 때문에 그룹 내 리더를 선출한다. 코디네이터는 컨슈머 B와 컨슈머 C에게 조인 요청에 대한 응답을 보낸다. 응답에는 어떤 컨슈머 그룹 내 리더인지 알 수 있는 정보가 포함되어 있다.
5. 리더는 컨슈머들의 메타데이터 등을 참고해서 각 컨슈머에게 파티션을 어떻게 할당할지 결정한다. 그리고 결정된 사항을 코디네이터에게 전달한다. 코디네이터는 새로운 파티션 할당 정보를 팔로워에게 전달함으로써 파티션 리밸런싱이 완료

위와 같은 상황은 poll 호출 주기가 길다면 충분히 발생할 수 있는 상황이다. 따라서 파티션 리밸런싱 시간을 단축하는 것이 좋으며, 이를 위해 max.poll.records 수를 줄이는 것이 도움이 될 수 있다.

**max.poll.records를 줄이더라도 성능에 큰 영향을 주지 않는 이유**

<img width="834" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/f692e1c4-0eb3-4432-a2ec-af896a88d829">

1. 컨슈머는 poll 메소드가 호출되면, Fetcher의 fetchedRecords 메소드를 호출합니다. fetchedRecords 메소드는 최대 max.poll.records 만큼의 레코드를 리턴합니다.
2. 만약에 Fetcher가 레코드를 가지고 있지 않다면, fetchedRecords 메소드는 빈 Map을 반환합니다. 그리고 빈 Map이 반환된 경우에만 컨슈머는 Fetcher#sendFetches 메소드를 호출합니다. sendFetches 메소드에서는 Fetcher가 브로커에게 요청을 보내 레코드를 가져옵니다. 하나의 요청으로 최대 fetch.max.bytes 크기만큼 가져올 수 있고, 파티션당 최대 max.partition.fetch.bytes 크기만큼 가져올 수 있습니다. 그리고 요청은 현재 컨슘하고 있는 파티션의 리더에게 모두 보냅니다. 예를 들어, 현재 컨슈머가 컨슘하고 있는 파티션이 0, 1이고 0번 파티션의 리더가 브로커 1, 1번 파티션의 리더가 브로커 2라면, 컨슈머는 브로커 1, 2에게 각각 요청을 보냅니다. 즉 2개의 요청을 보내게 됩니다.
3. Fetcher#sendFetches 메소드를 호출 후, 컨슈머는 또다시 fetchedRecords 메소드를 호출합니다. 그러면 Fetcher는 요청을 통해 가져온 레코드 중에서 최대 max.poll.records 만큼의 레코드를 반환합니다.
4. 컨슈머의 poll 메소드가 또다시 호출이 된 경우, 컨슈머는 Fetcher#fetchedRecords 메소드를 호출할 것입니다. 이전 요청을 통해 가져온 레코드가 아직 남아있다면, Fetcher는 브로커에게 요청을 보내지 않고 가지고 있는 레코드 중에서 max.poll.records 만큼의 레코드를 바로 반환합니다.

> Fetcher는 멤버 변수로 ConsumerNetworkClient를 가지고 있는데, 이는 비동기적으로 동작하여 컨슈머 내부의 Linked Queue에 데이터를 적재하고, Fetcher는 Linked Queue로부터 레코드를 읽어가는 것이다.

<img width="798" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/cf59011a-3f15-47f7-9a83-fb3e10e566b4">
