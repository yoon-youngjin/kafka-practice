# 카프카 커넥트 주요 구성요소

## Spooldir Source Connector 

파일 시스템으로 부터 구분자가 존재하는 파일을 읽어서 메시징화하여 카프카 토픽으로 전달하는 source connector

<img width="704" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/9d513f03-9f3c-4615-89a4-a87c7d674d66">

**Kafka Connect에 새로운 Connector 생성 순서 및 유의사항**

1. Connector를 다운로드 받음. 하나의 Connector는 보통 여러 개의 jar library들로 구성됨
2. 여러 개의 jar library로 구성된 하나의 Connector를 plugin.path로 지정된 디렉토리에 별도의 서브 디렉토리(ex. spooldir_source)로 만들어서 jar library들을 이동
3. Kafka Connect는 기동시에 plugin.path로 지정된 디렉토리 밑의 서브 디렉토리들에 위치한 모든jar 파일을 로딩함. 따라서 신규로 Connector를 Connect로 올릴 시에는 반드시 Connect를 재기동 해야 반영
4. Connector는 Connector명, Connector 클래스명, Connector 고유의 환경 설정 등을 REST API를 통해 Kafka Connect에 전달하여 새롭게생성
5. REST API에서 성공Response(HTTP 201)이 반환되더라도 Kafka Connect Log 메시지를 반드시 확인하여 제대로 Connector가 동작하는지 확인 필요

### REST API 기반의 Connect 관리

- GET: 기동 중인 모든 커넥터들의 리스트, 개별 커넥터의 config와 현재 상태
- POST: 커넥터 생성 등록, 개별 커넥터 재기동
- PUT: 커넥터 일시 정지 및 재시작, 커넥터의 새로운 config 등록, Config validation
- DELETE: 커넥터 삭제

<img width="653" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/4471d133-ea7f-438b-bcc2-10ecc9117aad">

`connector_configs/spooldir_source.json`

```json
{
  "name": "csv_spooldir_source",
  "config": {
    "tasks.max": "3",
    "connector.class": "com.github.jcustenborder.kafka.connect.spooldir.SpoolDirCsvSourceConnector",
    "input.path": "/home/min/spool_test_dir",
    "input.file.pattern": "^.*\\.csv",
    "error.path": "/home/min/spool_test_dir/error",
    "finished.path": "/home/min/spool_test_dir/finished",
    "empty.poll.wait.ms": 30000,
    "halt.on.error": "false",
    "topic": "spooldir-test-topic",
    "csv.first.row.as.header": "true",
    "schema.generation.enabled": "true"
   }
}
```
- name: 커넥터명
- tasks.max: 태스트 스레드 최대 개수
  - 최대 3개지 항상 3개는 아니다. (병렬 스레드를 지원하는 경우에만 적용된다.)
- connector.class: 커넥트에 등록할 커넥터 클래스명 (현재는 csv 파일을 활용하기 때문에 SpoolDirCsvSourceConnector를 사용한다.)
- input.path: 해당 path에 존재하는 파일을 읽는다.
- input.file.pattern: 정규식 표현 가능, *.csv 파일을 읽는다.
- error.path: 파일을 읽어들일 시 파일 포맷등의 오류가 발생할 때 해당 오류들을 저장하는 디렉토리
- finished.path: 소스 커텍터가 카프카 토픽으로 메시지 전송이 완료된 후 원래 파일을 이동시키는 디렉토리
  - input.path에 지정된 csv 파일을 바로 카프카 토픽으로 전송하고 해당 위치에 존재하는 csv파일을 finished 디렉토리로 이동한다. 
  - 이때 동일한 csv 파일을 input.path에 올리더라도 전송하지 않는다. (중복 전송 제거)
- empty.poll.wait.ms: 디렉토리에 파일이 있는지 모니터링하는 주기(ms 단위)
- halt.on.error: Delimter 파싱 오류 발생할 경우 커넥터 수행 중지 여부
- topic: 메시지를 전송할 토픽(없으면 auto create)
- csv.first.row.as.header: CSV 파일의 첫 라인이 헤더인지 여부
- schema.generation.enabled: 메시지 생성 시 스키마 포맷으로 생성할 지 여부 

```text
{"schema":{"type":"struct","fields":[],"optional":false,"name":"com.github.jcustenborder.kafka.connect.model.Key"},"payload":{}}	{"schema":{"type":"struct","fields":[{"type":"string","optional":true,"field":"id"},{"type":"string","optional":true,"field":"first_name"},{"type":"string","optional":true,"field":"last_name"},{"type":"string","optional":true,"field":"email"},{"type":"string","optional":true,"field":"gender"},{"type":"string","optional":true,"field":"ip_address"},{"type":"string","optional":true,"field":"last_login"},{"type":"string","optional":true,"field":"account_balance"},{"type":"string","optional":true,"field":"country"},{"type":"string","optional":true,"field":"favorite_color"}],"optional":false,"name":"com.github.jcustenborder.kafka.connect.model.Value"},"payload":{"id":"1000","first_name":"Phineas","last_name":"Frede","email":"pfrederr@weather.com","gender":"Male","ip_address":"59.83.98.78","last_login":"2015-12-04T22:18:07Z","account_balance":"14095.22","country":"PK","favorite_color":"#4f2f2b"}}
```

**정리**

- 필요한 커넥터 다운로드후 connector-distributed.properties에 지정한 plugin_path에 서브 디렉토리(ex. spooldir_source)를 만들어서 *.jar 파일을 둔다.
- 해당 커넥터에 필요한 config 파일(ex. spooldir_source.json)을 만든다.
  - config 파일에 error.path, finished.path를 지정하는데 미리 폴더를 만들어두지 않으면 에러 발생
- 해당 config 파일을 통해 커넥트에 커넥터를 등록한다. (curl -X POST -H "Content-Type: application/json" http://localhost:8083/connectors --data @spooldir_source.json)
  - 커넥터를 등록할 때 지정한 토픽이 없으면 auto create 

## 커넥터 생성 시 커넥트(Connect)의 내부 프로세스 수행 이해

### Connect, Connector, Worker, Task 정의

- Connect는 커넥터를 기동 시키기 위한 프레임워크를 갖춘 JVM Process Model. (Connect Process를 Worker Process로 지칭)
- Connect는 REST API를 통해 서로 다른 여러 개의 Connector 인스턴스(클래스)들을 자신의 프레임워크 내부로 로딩하고 호출/수행 시킬 수 있음
- Connector 인스턴스의 실제 수행은 쓰레드 레벨로 수행되며 이를 Taks라고 한다. Connector가 병렬 쓰레드 수행이 가능할 경우 여러 개의 태스크 쓰레드들로 해당 커넥터를 수행할 수 있다.

### Source Connect에서 Connector 등록 생성 시 수행 프로세스

SpoolDir Source 커넥터를 커넥트에 등록하는 상황 

<img width="743" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/977174da-2818-4162-ad5d-edc2f92810d8">

Connector Thread가 먼저 생성되어서 Task 환경 설정 관리 및 task.max에 기술된 숫자만큼 task thread 생성 호출 (병렬을 지원한다면 생성)

- 커넥터를 생성할 때 start() 및 poll()을 구현하여 src system(csv file, ...)으로 부터 주기적으로 모니터링하여 메시지를 가져온다.
- 주기적으로 가져온 데이터는 프로듀서가 브로커로 보내는데, 프로듀서는 커넥터가 아닌 커넥트에서 관리된다.

## 스키마 메시지의 이해와 필요성

`schema.generation.enabled` 옵션에 따라서 컨버터가 컨버팅을 하는데 기본은 json 포맷이다.
이러한 컨버터는 커넥트 프레임워크에 포함되어있다.

`connect-distributed.properties`

```text
...
key.converter.schemas.enable=true
value.converter.schemas.enable=true
...
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
```

<img width="430" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/4261098e-f87c-4ab1-ad27-db715411bdee">

프로듀서와 컨슈머 간에는 Serialized Message를 전송하는데, 이렇게 직렬화 된 Byte Array를 주고 받음으로써 네트웍을 통해 손쉽게 데이터를 전송할 수 있다.

### 스키마 필요성 

<img width="747" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/c03be0c4-d57f-413e-91a2-4fd5c93431af">

위와 같은 String을 역직렬화해서 컨슈머에서 타겟 DB에 입력하는 로직을 만들어야 한다면 각 타입을 모두 알아야한다. 
따라서 이를 위해 이전 강의에서는 자바 객체를 이용했다.

**자바 객체를 이용하여 스키마가 있는 정보 전송 시 문제**

- 수많은 테이블의 스키마 정보를 모두 커스텀 자바 객체로 만들기 어렵다.
- 스키마 정보가 추가/변경/삭제 시 마다 커스텀 자바 객체를 변경해야 한다.

하지만 카프카 커넥트이 연동 대상은 대부분 스키마 기반의 시스템이다.
따라서, 연동 시 스키마를 기반으로 효율적으로 소스 데이타와 싱크 데이터를 표현할 수 있는 별도의 공통 방식(포맷)이 필요하다. 
**이를 위해 커넥트에는 컨버터가 존재한다**

## 컨버터의 이해 

<img width="763" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/326ca687-5b87-4c4e-914e-67fa904e55a9">

- 어떤 포맷(json, avro, ...)으로 지정할지 커넥터 설정에 포함되어 있지만 해당 설정을 컨버터에도 지정할 수 있으며 컨버터에 지정한 설정으로 overwrite된다.
  - 지원 포맷: Json, Avro, Protobuf, String, ByteArray
  - 일반적으로 json 타입을 많이 사용하고, 운영 환경에서는 Avro를 많이 사용
  - schema: 해당 레코드의 schema 구성을 payload는 해당 레코드의 값을 가진다.
  - Json Schema의 경우 레코드 별로 Schema를 가지고 있으므로 메시지 용량이 커진다. 이의 개선을 위해 Avro Format과 Schema Registry를 이용하여 Schema 정보의 중복 생성을 제거한다.

```json
{
  "schema": {
    "type": "struct",
    "fields": [
      {
        "type": "string",
        "optional": true,
        "field": "id"
      },
      {
        "type": "string",
        "optional": true,
        "field": "first_name"
      },
      {
        "type": "string",
        "optional": true,
        "field": "last_name"
      },
      {
        "type": "string",
        "optional": true,
        "field": "email"
      },
      {
        "type": "string",
        "optional": true,
        "field": "gender"
      },
      {
        "type": "string",
        "optional": true,
        "field": "ip_address"
      },
      {
        "type": "string",
        "optional": true,
        "field": "last_login"
      },
      {
        "type": "string",
        "optional": true,
        "field": "account_balance"
      },
      {
        "type": "string",
        "optional": true,
        "field": "country"
      },
      {
        "type": "string",
        "optional": true,
        "field": "favorite_color"
      }
    ],
    "optional": false,
    "name": "com.github.jcustenborder.kafka.connect.model.Value"
  },
  "payload": {
    "id": "1000",
    "first_name": "Phineas",
    "last_name": "Frede",
    "email": "pfrederr@weather.com",
    "gender": "Male",
    "ip_address": "59.83.98.78",
    "last_login": "2015-12-04T22:18:07Z",
    "account_balance": "14095.22",
    "country": "PK",
    "favorite_color": "#4f2f2b"
  }
}
```

### Schema Registry를 이용한 Schema 정보 중복 전송 제거

- Confluent Kafka는 Schema Registry를 통해 Schema 정보를 별도로 관리하는 기능을 제공
- 토픽으로 전송되는 Data의 Schema는 Schema Registry에서 ID + Version 별로 중앙 관리되므로 레코드 별로 Schema를 중복해서 전송할 필요가 없다.

<img width="667" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/f63612e2-89e1-4fc5-8964-4c3a5562cd20">

## 커넥트 내부 토픽 이해 - connect-offsets, connect-status, connect-configs

Connect는 커넥터의 메시지 처리 오프셋 및 커넥터 별 Config와 상태 정보를 내부 토픽에 저장한다.

- connect-offsets: 소스 커넥터 별로 메시지를 전송한 오프셋 정보를 가지고 있다.
  - 이로인해 한번 전송한 메시지를 중복 전송하지 않는다.
  - 기본 25개의 파티션 구성 (connect-offsets-24)
  - 전송한 파일명(csv-sample-01.csv)으로 어디까지 전송했는지 오프셋 정보를 가지고 있다.
- connect-configs: 커넥터의 config 정보를 가진다. connect 재기동 시 설정된 커넥트를 기동
- connect-status: 커넥터의 상태 정보를 가진다. connect 재기동 시 설정된 커넥터를 기동
- __consumer_offsets: 컨슈머가 읽어들인 메시지의 오프셋 정보를 가지고 있다. Sink 커넥터가 읽어 들인 메시지 오프셋 정보를 가지고 있다. 
  - 한번 읽어 들인 메시지를 중복해서 읽지 않기 위함 
  - 기본 50개의 파티션으로 구성 (__consumer_offsets_49)
- connect-offsets, connect-configs, connect-status 해당 토픽명과 파티션 개수는 설정 파일에서 수정이 가능하다.

## connect-offsets 내부 토픽을 이용한 소스 커넥터의 offset 관리 메커니즘 이해

```text
kafka-console-consumer --bootstrap-server localhost:9092 --topic connect-offsets --from-beginning --property print.key=true 
```

```text
["csv_spooldir_source",{"fileName":"csv-spooldir-source.csv"}]	{"offset":1000}
```
- key: "csv_spooldir_source",{"fileName":"csv-spooldir-source.csv"}
- value : "offset":1000

connect-offsets 내부 토픽에는 소스 커넥터, 파일명 단위로 오프셋을 관리하여, 소스 커넥터에 동일한 파일명에 대한 파일을 중복처리 하지않는다.
그리고 config에 지정한 토픽명에 실제 데이터는 적재된다. 

> spooldir 소스 커넥터는 기동 시점에 지정한 형식에 맞는 최초 파일이 폴더에 존재해야한다.

## SMT(Single Message Transform) 이해

Transform은 카프카 커넥트의 주요 구성 요소 중 하나이다.(커넥터, 트랜스폼, 컨버터) 
- 커넥트에서 메시지 변환을 위해 제공하는 라이브러리 

### SMT를 통한 메시지 변환

<img width="770" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/056b0217-6955-4ca4-8d29-b6e44564556f">

소스 타겟에서 DML(Insert, Update, ...)이 발생하여 소스 커넥터가 데이터를 가져온 뒤 트랜스폼을 진행한다.
- 컬럼, 속성 추가, Record 필터링, 포맷팅, ... 
- 트랜스폼은 chain 형식으로 가능하다.
- 트랜스폼은 Config를 통해 적용한다.

### SMT 특징

- 소스 시스템의 메시지를 카프카로 전송 전에 변환하거나 카프카에서 싱크 시스템으로 데이터를 입력하기 전에 변환할 때 적용
- 커넥트는 커넥터와 Config 만으로 인터페이스 하므로 메시지 변환도 Config에서 SMT를 기술하여 적용해야한다.
- SMT가 제공하는 변환 기능은 SMT를 구현한 자바 클래스를 기반으로 제공되며, 카프카 커넥트는 기본적으로 SMT 클래스를 가지고 있다.
- 커넥트에서 기본제공하는 SMT 클래스 외에 3rd party에서 별도의 SMT를 플러그인 형태로 제공할 수 있다.
- 체인 형태로 연속해서 적용 가능하다.
- 복잡한 데이터 변환은 한계가 있다.

<img width="656" alt="image" src="https://github.com/yoon-youngjin/spring-study/assets/83503188/afb98725-899f-4d7c-bb7c-dfe65d992bfa">
